{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Algoritmos_CUDA_SCP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugernaut/ProgramacionEnParalelo/blob/desarrollo/CUDA/02_Algoritmos_CUDA_SCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rYgUDA8Ltm4"
      },
      "source": [
        "<font color=\"Teal\" face=\"Comic Sans MS,arial\">\n",
        "  <h1 align=\"center\"><i>Algoritmos CUDA</i></h1>\n",
        "  </font>\n",
        "  <font color=\"Black\" face=\"Comic Sans MS,arial\">\n",
        "  <h5 align=\"center\"><i>Profesor: M.en.C. Miguel Angel Pérez León</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Jesús Iván Coss Calderón</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Mario Arturo Nieto Butron</i></h5>\n",
        "  <h5 align=\"center\"><i>Materia: Seminario de programación en paralelo</i></h5>\n",
        "  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTIY01-ELvbH"
      },
      "source": [
        "# Introducción\n",
        "\n",
        "Como ya se menciono previamente, *CUDA* hace uso de las *GPU's* (dispositivos de cómputo de propósito especifico), estos dispositivos están optimizados para trabajar con imágenes y resulta que una imagen dentro de una computadora se representa mediante elementos matemáticos como matrices o vectores.\n",
        "\n",
        "De hecho la mayoría de los formatos de imágenes más comunes (*.jpg, .jpeg, .png*) consideran a la imagen como una matriz de pixeles o un mapa de bits.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/ProgramacionEnParalelo/blob/desarrollo/Imagenes/CUDA/smile.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyAG-V_3MwrN"
      },
      "source": [
        "# Suma de Vectores\n",
        "\n",
        "El algoritmo más sencillo que podemos comenzar a analizar es la suma de vectores y aunque suene trivial, la realidad es que hasta antes de este momento toda suma de vectores que hayas realizado previamente **se ejecuto de manera secuencial**, lo cuál significa un desperdicio de recursos.\n",
        "\n",
        "Gracias a *CUDA* (aunque también se puede realizar con *OpenMP* y *MPI*) esta operación elemental se puede realizar en paralelo y gracias a esto optimizar recursos, lo que se traduce en un menor tiempo de ejecución."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6YKyaVxM19I"
      },
      "source": [
        "## ¿Cómo funciona?\n",
        "\n",
        "La forma tradicional de como funciona la suma de vectores, se basa en la definición formal \"se realiza entrada a entrada\", sin embargo nunca nos dijeron que está \"suma entrada a entrada\" se puede realizar en paralelo.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/ProgramacionEnParalelo/blob/desarrollo/Imagenes/CUDA/sumavect.png?raw=1\" width=\"600\"> \n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSg1edoNAwsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "840c3af0-1e03-4f81-a87d-fc20ac542b33"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-3whjiloi\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-3whjiloi\n",
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HGthAMmxdLf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9703413-edce-4210-cd81-3d10ddb57715"
      },
      "source": [
        "%%cu\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <stdio.h>\n",
        "\n",
        "// Kernel (funcion) que se invoca desde el Host y se ejecuta en un dispositivo\n",
        "__global__ void suma_vectores(int* c, const int* a, const int* b, int size) {\n",
        "    // polinomio de direccionamiento\n",
        "    // ¡¡OJO 2 bloques(0 y 1), dim = 3, thread de 0-2!!\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < size) {\n",
        "        //printf(\"%d \\n\",blockIdx.x);\n",
        "        //printf(\"%d \\n\",blockDim.x);\n",
        "        //printf(\"%d \\n\",threadIdx.x);\n",
        "        c[i] = a[i] + b[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Funcion auxiliar que encapsula la suma con CUDA\n",
        "void suma_CUDA(int* c, const int* a, const int* b, int tam) {\n",
        "    int* dev_a = nullptr;\n",
        "    int* dev_b = nullptr;\n",
        "    int* dev_c = nullptr;\n",
        "\n",
        "    // Reservamos espacio de memoria para los datos, 2 de entrada y una salida\n",
        "    cudaMalloc((void**)&dev_c, tam * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_a, tam * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_b, tam * sizeof(int));\n",
        "\n",
        "    // Copiamos los datos de entrada desde el CPU a la memoria del GPU\n",
        "    cudaMemcpy(dev_a, a, tam * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, tam * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Se invoca al kernel en el GPU con un hilo por cada elemento\n",
        "    // 2 es el numero de bloques y (tam + 1)/2 es el numero de hilos en cada bloque\n",
        "    suma_vectores<<<2, (tam + 1) / 2>>>(dev_c, dev_a, dev_b, tam);\n",
        "    \n",
        "    // Esta funcion espera a que termine de ejecutarse el kernel y \n",
        "    // devuelve los errores que se hayan generado al ser invocado\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copiamos el vector resultado de la memoria del GPU al CPU\n",
        "    cudaMemcpy(c, dev_c, tam * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Se libera la memoria empleada\n",
        "    cudaFree(dev_c);\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "}\n",
        "\n",
        "// Funcion principal que sirve de prueba para el algoritmo\n",
        "int main(int argc, char** argv) {\n",
        "    \n",
        "    // Datos de entrada para nuestra funcion\n",
        "    const int tam = 5;\n",
        "    const int a[tam] = {  1,  2,  3,  4,  5 };\n",
        "    const int b[tam] = { 10, 20, 30, 40, 50 };\n",
        "    int c[tam] = { 0 };\n",
        "\n",
        "    // Se llama a la funcion que encapsula el Kernel\n",
        "    suma_CUDA(c, a, b, tam);\n",
        "\n",
        "    // Mostramos resultado\n",
        "    printf(\"{1, 2, 3, 4, 5} + {10, 20, 30, 40, 50} = {%d, %d, %d, %d, %d}\\n\", c[0], c[1], c[2], c[3], c[4]);\n",
        "\n",
        "    // Se liberan recursos\n",
        "    cudaDeviceReset();\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1, 2, 3, 4, 5} + {10, 20, 30, 40, 50} = {11, 22, 33, 44, 55}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A57hiA_xM7DU"
      },
      "source": [
        "## Ventajas\n",
        "\n",
        "Debido a lo que ya conocemos respecto al funcinamiento y desempeño de *CUDA*, podemos afirmar que este tipo de operaciones (suma de vectores) se realizan de manera más sencilla en un *GPU*.\n",
        "\n",
        "Sean $\\vec{a}=\\{1,2,3,4,5\\}$ y $\\vec{b}=\\{10,20,30,40,50\\}$ entonces $\\left(a_{0},b_{0}\\right)$ se envían al núcleo 0, $\\left(a_{1},b_{1}\\right)$ se envían al núcleo 1, así sucesivamente hasta $\\left(a_{n-1},b_{n-1}\\right)$ se envían al núcleo $n-1$. En caso de que existan más entradas que nucleos, entonces se tendría que esperar a liberar alguno de los núcleos previamente empleados, sin embargo debido a la arquitectura de los *GPU's* sabemos que cada *GPU* posee muchos nucleos, **en las tarjetas más recientes podemos hablar del orden de miles**.\n",
        "\n",
        "De igual manera como se midió el tiempo de ejecución con *OpenMP* o *MPI*, existe forma de medir el tiempo de ejecución de los algoritmos usando *CUDA*.\n",
        "\n",
        "¿Cuáles son los ordenedes de complejidad a los que pertenecen ambas versiones de la suma de vectores, secuencial y en paralelo?.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4MYFdgKNDcB"
      },
      "source": [
        "## Desventajas\n",
        "\n",
        "La desventaja más notoria en el algoritmo anterior, es el cuello de botella que se genera tanto al enviar los datos a procesar al *GPU*, como al extraerlos del *GPU*, sin embargo es claro que a mayor cantidad de datos a procesar también se tendría una mayor ganancia en tiempo, lo que se traduce en un menor tiempo de ejecución.\n",
        "\n",
        "Este tipo de características (gran cantidad de datos a procesar) normalmente se encuentran en muchas áreas de las ciencias e ingenierías, por ejemplo ***machine learning***, por mencionar alguna.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WogAVZ_eNKKm"
      },
      "source": [
        "# Operaciones con matrices\n",
        "\n",
        "Una vez que ya se comprendió el desempeño de *CUDA* para la suma de vectores, es sencillo extender su aplicación a operaciones con matrices, por ejemplo la **suma de matrices**.\n",
        "\n",
        "Por lo general cuando se hace uso de modelos matemáticos, estos toman la forma de matriz, uno de los más conocidos son las redes neuronales. Estas redes neuronales se representan por una matriz y en cada entrada de la matriz se tiene una neurona, que a su vez cada neurona puede ser representada por un escalar o un vector o incluso otra matriz.\n",
        "\n",
        "Es por este motivo que las *GPU's* y en particular *CUDA* ha mostrado un excelente desempeño en el proceso de entrenamiento de las redes neuronales, recientemente han surgido dispositivos de computo especifico como los *TPU's* (unidades de procesamiento tensorial).\n",
        "\n",
        "Por este motivo la jerarquía de memoria de *CUDA* se estructura de la siguiente manera.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/memtotal.png?raw=1\" width=\"600\"> \n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9kPwGmoNPbY"
      },
      "source": [
        "## Entendiendo CUDA\n",
        "\n",
        "Queda como ejercicio, realizar la suma de matrices empleando *CUDA*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsmbEzMJlByi"
      },
      "source": [
        "## Hint (polinomio de direccionamiento)\n",
        "\n",
        "Imaginemos que queremos \"aplanar\" una matríz para representarla con un vector, es decir que necesitamos almacenar (y recuperar) cada una de las entradas de una matriz en un vector, ¿cómo le hacemos?.\n",
        "\n",
        "La respuesta es mediante el polinomio de direccionamiento, este polinomio nos indica mediante una sola entrada, la localidad del vector que le corresponde a cada uno de los elementos de una matriz.\n",
        "\n",
        "Supongamos que queremos almacenar los elementos de la matriz $A$ en una lista o vector.\n",
        "\n",
        "Sea\n",
        "\n",
        "$$A\\in M_{2x2}=\\left(\\begin{array}{cc}\n",
        "3_{(0,0)} & 6_{(0,1)}\\\\\n",
        "7_{(1,0)} & 9_{(1,1)}\n",
        "\\end{array}\\right)$$\n",
        "\n",
        "La idea sería que estos elementos se alamcenen de la siguiente forma.\n",
        "\n",
        "$$A=\\left[\\begin{array}{cccc}\n",
        "3 & 6 & 7 & 9\\end{array}\\right]$$\n",
        "\n",
        "Nos gustaría que la entrada $(0,0)$ de $A$ fuera mapeada a la localidad 0 de la lista y así sucesivamente hasta llegar a que la entrada $(1,1)$ se mapeara a la localidad 3 del arreglo, es decir\n",
        "\n",
        "\\begin{array}{cc}\n",
        "f((0,0))=0 & f((0,1))=1\\\\\n",
        "f((1,0))=2 & f((1,1))=3\n",
        "\\end{array}\n",
        "\n",
        "Podríamos pensar que una buena forma de definir a $f$, seria $f((x,y))=x+y$, pero veamos que sucede al probarla.\n",
        "\n",
        "\\begin{array}{c}\n",
        "f((0,0))=0+0=0.......\\text{¡bien!}\\\\\n",
        "f((0,1))=0+1=1.......\\text{¡bien!}\n",
        "\\end{array}\n",
        "\n",
        "Vamos bien, veamos que sicede con los elementos restantes.\n",
        "\n",
        "\\begin{array}{c}\n",
        "f((1,0))=1+0=1.......\\text{¡colisión!}\\\\\n",
        "f((0,1))=1=f((1,0))\n",
        "\\end{array}\n",
        "\n",
        "Dado que se tuvo una colisión, es necesario re-definirla de otra manera menos ingenua. Veamos que sucede si definimos a $f$ de la siguiente manera.\n",
        "\n",
        "$$f((x,y))=2x+y$$\n",
        "\n",
        "Al probarla, lo que obtenemos es.\n",
        "\n",
        "\\begin{array}{c}\n",
        "f((0,0))=2*0+0=0\\\\\n",
        "f((0,1))=2*0+1=1\\\\\n",
        "f((1,0))=2*1+0=2\\\\\n",
        "f((1,1))=2*1+1=3\n",
        "\\end{array}\n",
        "\n",
        "Esta función, no muestra colisiones (al menos en el dominio y codominio definidos), incluso se podría probar que no presentará colisiones para ningún par de tuplas de naturales.\n",
        "\n",
        "Así que podemos pensar, que para el caso particular de matrices bidimensionales $A_{(i,j)}\\in M_{ren\\ \\times col}$ podemos definir la función hash (polinomio de direccionamiento) que mapea localidades de dicha matriz en una lista (arreglo) unidimensional de la siguiente forma.\n",
        "\n",
        "$$f((i,j))=col*i+j$$\n",
        "\n",
        "¿Podemos extender este polinomio a objetos de 3 dimensiones, largo, ancho, profundidad?."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfteTdToPWyV"
      },
      "source": [
        "# Glosario\n",
        "\n",
        "*Host*: En el contexto de *CUDA* el host es el *CPU* del dispositivo de cómputo en el que se ejecuta el algoritmo.\n",
        "\n",
        "Asíncrono: En computación un evento (proceso) asíncrono es aquel no tiene correspondencia temporal con otro evento. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHU_RuNSPbRQ"
      },
      "source": [
        "# Referencias\n",
        "\n",
        "1. Tolga Soyata: GPU Parallel Program Development Using Cuda.\n",
        "2. https://fisica.cab.cnea.gov.ar/gpgpu/images/clases/clase_1_cuda.pdf\n",
        "3. Dongarra Foster: Source Book of parallel computing."
      ]
    }
  ]
}