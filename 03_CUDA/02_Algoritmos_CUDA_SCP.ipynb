{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugernaut/MACTI-programacionparalelo/blob/erick/03_CUDA/02_Algoritmos_CUDA_SCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rYgUDA8Ltm4"
      },
      "source": [
        "<font color=\"Teal\" face=\"Comic Sans MS,arial\">\n",
        "  <h1 align=\"center\"><i>Algoritmos CUDA</i></h1>\n",
        "  </font>\n",
        "  <font color=\"Black\" face=\"Comic Sans MS,arial\">\n",
        "  <h5 align=\"center\"><i>Profesor: M.en.C. Miguel Angel Pérez León</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Jesús Iván Coss Calderón</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Mario Arturo Nieto Butron</i></h5>\n",
        "  <h5 align=\"center\"><i>Materia: Seminario de programación en paralelo</i></h5>\n",
        "  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTIY01-ELvbH"
      },
      "source": [
        "# Introducción\n",
        "\n",
        "Como ya se menciono previamente, *CUDA* hace uso de las *GPU's* (dispositivos de cómputo de propósito especifico), estos dispositivos están optimizados para trabajar con imágenes y resulta que una imagen dentro de una computadora se representa mediante elementos matemáticos como matrices o vectores.\n",
        "\n",
        "De hecho la mayoría de los formatos de imágenes más comunes (*.jpg, .jpeg, .png*) consideran a la imagen como una matriz de pixeles o un mapa de bits.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/ProgramacionEnParalelo/blob/desarrollo/Imagenes/CUDA/smile.png?raw=1\" width=\"600\">\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyAG-V_3MwrN"
      },
      "source": [
        "# Suma de Vectores\n",
        "\n",
        "El algoritmo más sencillo que podemos comenzar a analizar es la suma de vectores y aunque suene trivial, la realidad es que hasta antes de este momento toda suma de vectores que hayas realizado previamente **se ejecuto de manera secuencial**, lo cuál significa un desperdicio de recursos.\n",
        "\n",
        "Gracias a *CUDA* (aunque también se puede realizar con *OpenMP* y *MPI*) esta operación elemental se puede realizar en paralelo y gracias a esto optimizar recursos, lo que se traduce en un menor tiempo de ejecución."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6YKyaVxM19I"
      },
      "source": [
        "## ¿Cómo funciona?\n",
        "\n",
        "La forma tradicional de como funciona la suma de vectores, se basa en la definición formal \"se realiza entrada a entrada\", sin embargo nunca nos dijeron que esta \"suma entrada a entrada\" se puede realizar en paralelo.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/ProgramacionEnParalelo/blob/desarrollo/Imagenes/CUDA/sumavect.png?raw=1\" width=\"600\">\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSg1edoNAwsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "705befe0-084d-4279-b7a7-2987a10dba47"
      },
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-nq4w423i\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-nq4w423i\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HGthAMmxdLf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d167c2-62f4-40b2-ef4b-fb6533d39224"
      },
      "source": [
        "%%cu\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <stdio.h>\n",
        "\n",
        "// Kernel (funcion) que se invoca desde el Host y se ejecuta en un dispositivo\n",
        "__global__ void suma_vectores(int* c, const int* a, const int* b, int size) {\n",
        "    // polinomio de direccionamiento\n",
        "    // ¡¡OJO 2 bloques(0 y 1), dim = 3, thread por bloque de 0-2!!\n",
        "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (i < size) {\n",
        "        // descomnetar para debugear\n",
        "        //printf(\"%d \\n\",blockIdx.x);\n",
        "        //printf(\"%d \\n\",blockDim.x);\n",
        "        //printf(\"%d \\n\",threadIdx.x);\n",
        "        c[i] = a[i] + b[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Funcion auxiliar que encapsula la suma con CUDA\n",
        "void suma_CUDA(int* c, const int* a, const int* b, int tam) {\n",
        "    int* dev_a = nullptr;\n",
        "    int* dev_b = nullptr;\n",
        "    int* dev_c = nullptr;\n",
        "\n",
        "    // Reservamos espacio de memoria para los datos, 2 de entrada y una salida\n",
        "    cudaMalloc((void**)&dev_c, tam * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_a, tam * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_b, tam * sizeof(int));\n",
        "\n",
        "    // Copiamos los datos de entrada desde el CPU a la memoria del GPU\n",
        "    cudaMemcpy(dev_a, a, tam * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, tam * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Se invoca al kernel en el GPU con un hilo por cada elemento\n",
        "    // 2 es el numero de bloques y (tam + 1)/2 es el numero de hilos en cada bloque\n",
        "    suma_vectores<<<2, (tam + 1) / 2>>>(dev_c, dev_a, dev_b, tam);\n",
        "\n",
        "    // Esta funcion espera a que termine de ejecutarse el kernel y\n",
        "    // devuelve los errores que se hayan generado al ser invocado\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copiamos el vector resultado de la memoria del GPU al CPU\n",
        "    cudaMemcpy(c, dev_c, tam * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Se libera la memoria empleada\n",
        "    cudaFree(dev_c);\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "}\n",
        "\n",
        "// Funcion principal que sirve de prueba para el algoritmo\n",
        "int main(int argc, char** argv) {\n",
        "\n",
        "    // Datos de entrada para nuestra funcion\n",
        "    const int tam = 5;\n",
        "    const int a[tam] = {  1,  2,  3,  4,  5 };\n",
        "    const int b[tam] = { 10, 20, 30, 40, 50 };\n",
        "    int c[tam] = { 0 };\n",
        "\n",
        "    // Se llama a la funcion que encapsula el Kernel\n",
        "    suma_CUDA(c, a, b, tam);\n",
        "\n",
        "    // Mostramos resultado\n",
        "    printf(\"{1, 2, 3, 4, 5} + {10, 20, 30, 40, 50} = {%d, %d, %d, %d, %d}\\n\", c[0], c[1], c[2], c[3], c[4]);\n",
        "\n",
        "    // Se liberan recursos\n",
        "    cudaDeviceReset();\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1, 2, 3, 4, 5} + {10, 20, 30, 40, 50} = {11, 22, 33, 44, 55}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A57hiA_xM7DU"
      },
      "source": [
        "## Ventajas\n",
        "\n",
        "Debido a lo que ya conocemos respecto al funcinamiento y desempeño de *CUDA*, podemos afirmar que este tipo de operaciones (suma de vectores) se realizan de manera más sencilla en un *GPU*.\n",
        "\n",
        "Sean $\\vec{a}=\\{1,2,3,4,5\\}$ y $\\vec{b}=\\{10,20,30,40,50\\}$ entonces $\\left(a_{0},b_{0}\\right)$ se envían al núcleo 0, $\\left(a_{1},b_{1}\\right)$ se envían al núcleo 1, así sucesivamente hasta $\\left(a_{n-1},b_{n-1}\\right)$ se envían al núcleo $n-1$. En caso de que existan más entradas que nucleos, entonces se tendría que esperar a liberar alguno de los núcleos previamente empleados, sin embargo debido a la arquitectura de los *GPU's* sabemos que cada *GPU* posee muchos nucleos, **en las tarjetas más recientes podemos hablar del orden de miles**.\n",
        "\n",
        "De igual manera como se midió el tiempo de ejecución con *OpenMP* o *MPI*, existe forma de medir el tiempo de ejecución de los algoritmos usando *CUDA*.\n",
        "\n",
        "¿Cuáles son los ordenedes de complejidad a los que pertenecen ambas versiones de la suma de vectores, secuencial y en paralelo?.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/gpucpu.jpg?raw=1\" width=\"600\">\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4MYFdgKNDcB"
      },
      "source": [
        "## Desventajas\n",
        "\n",
        "La desventaja más notoria en el algoritmo anterior, es el **cuello de botella** que se genera tanto al enviar los datos a procesar al *GPU*, como al extraerlos del *GPU*, sin embargo es claro que a mayor cantidad de datos a procesar también se tendría una mayor ganancia en tiempo, lo que se traduce en un menor tiempo de ejecución.\n",
        "\n",
        "Este tipo de características (gran cantidad de datos a procesar) normalmente se encuentran en muchas áreas de las ciencias e ingenierías, por ejemplo ***machine learning***, por mencionar alguna."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WogAVZ_eNKKm"
      },
      "source": [
        "# Operaciones con matrices\n",
        "\n",
        "Una vez que ya se comprendió el desempeño de *CUDA* para la suma de vectores, es sencillo extender su aplicación a operaciones con matrices, por ejemplo la **suma de matrices**.\n",
        "\n",
        "Por lo general cuando se hace uso de modelos matemáticos, estos toman la forma de matriz, uno de los más conocidos son las **redes neuronales**. Estas redes neuronales toman forma de matriz y en cada entrada de la matriz se tiene una neurona, y a su vez cada neurona puede ser representada por un escalar o un vector o incluso otra matriz.\n",
        "\n",
        "Es por este motivo que las *GPU's* y en particular *CUDA* ha mostrado un excelente desempeño en el proceso de entrenamiento de las redes neuronales, más recientemente han surgido nuevos dispositivos de cómputo especifico como los *TPU's* (unidades de procesamiento tensorial), dispositivos que desde que fueron diseñados se contemplo su uso para el área de inteligencia artificial.\n",
        "\n",
        "Volviendo al funcionamiento de *CUDA*, veamos una parte fundamental del mismo, la **jerarquía de memoria** de *CUDA* se estructura de la siguiente manera.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/memtotal.png?raw=1\" width=\"600\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9kPwGmoNPbY"
      },
      "source": [
        "## Entendiendo CUDA\n",
        "\n",
        "Queda como ejercicio, realizar la suma de matrices empleando *CUDA*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsmbEzMJlByi"
      },
      "source": [
        "## Hint (polinomio de direccionamiento)\n",
        "\n",
        "Imaginemos que queremos \"aplanar\" una matríz para representarla con un vector, es decir que necesitamos almacenar (y recuperar) cada una de las entradas de una matriz en un vector, ¿cómo le hacemos?.\n",
        "\n",
        "La respuesta es mediante el **polinomio de direccionamiento**, este polinomio nos indica mediante una sola entrada, la localidad del vector que le corresponde a cada uno de los elementos de una matriz.\n",
        "\n",
        "Supongamos que queremos almacenar los elementos de la matriz $A$ en una lista o vector.\n",
        "\n",
        "Sea\n",
        "\n",
        "$$A\\in M_{2x2}=\\left(\\begin{array}{cc}\n",
        "3_{(0,0)} & 6_{(0,1)}\\\\\n",
        "7_{(1,0)} & 9_{(1,1)}\n",
        "\\end{array}\\right)$$\n",
        "\n",
        "La idea sería que estos elementos se alamcenen de la siguiente forma.\n",
        "\n",
        "$$A=\\left[\\begin{array}{cccc}\n",
        "3 & 6 & 7 & 9\\end{array}\\right]$$\n",
        "\n",
        "Nos gustaría que la entrada $(0,0)$ de $A$ fuera mapeada a la localidad 0 de la lista y así sucesivamente hasta llegar a que la entrada $(1,1)$ se mapeara a la localidad 3 del arreglo, es decir\n",
        "\n",
        "\\begin{array}{cc}\n",
        "f((0,0))=0 & f((0,1))=1\\\\\n",
        "f((1,0))=2 & f((1,1))=3\n",
        "\\end{array}\n",
        "\n",
        "Podríamos pensar que una buena forma de definir a $f$, seria $f((x,y))=x+y$, pero veamos que sucede al probarla.\n",
        "\n",
        "\\begin{array}{c}\n",
        "f((0,0))=0+0=0.......\\text{¡bien!}\\\\\n",
        "f((0,1))=0+1=1.......\\text{¡bien!}\n",
        "\\end{array}\n",
        "\n",
        "Vamos bien, veamos que sucede con los elementos restantes.\n",
        "\n",
        "\\begin{array}{c}\n",
        "f((1,0))=1+0=1.......\\text{¡colisión!}\\\\\n",
        "f((0,1))=1=f((1,0))\n",
        "\\end{array}\n",
        "\n",
        "Dado que se tuvo una colisión, es necesario re-definirla de otra manera menos ingenua. Veamos que sucede si definimos a $f$ de la siguiente manera.\n",
        "\n",
        "$$f((x,y))=2x+y$$\n",
        "\n",
        "Al probarla, lo que obtenemos es.\n",
        "\n",
        "\\begin{array}{c}\n",
        "f((0,0))=2*0+0=0\\\\\n",
        "f((0,1))=2*0+1=1\\\\\n",
        "f((1,0))=2*1+0=2\\\\\n",
        "f((1,1))=2*1+1=3\n",
        "\\end{array}\n",
        "\n",
        "Esta función no muestra colisiones (al menos en el dominio y codominio definidos), incluso se podría probar que no presentará colisiones para ningún par de tuplas de naturales.\n",
        "\n",
        "Así que podemos pensar, que para el caso particular de matrices bidimensionales $A_{(i,j)}\\in M_{ren \\times col}$ podemos definir la función hash (polinomio de direccionamiento) que mapea localidades de dicha matriz en una lista (arreglo) unidimensional de la siguiente forma.\n",
        "\n",
        "$$f((i,j))=col*i+j$$\n",
        "\n",
        "¿Podemos extender este polinomio a objetos de 3 dimensiones, largo, ancho, profundidad?."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3CHYFvqZz79c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeCZ8FZf8K1I"
      },
      "source": [
        "## *Kernels*\n",
        "\n",
        "Una función del tipo **Kernel** es una función de *GPU* que debe llamarse desde el código de la *CPU*. Esta tiene dos características fundamentales:\n",
        "\n",
        "1.   Las funciones de tipo **Kernel** no pueden devolver explícitamente un valor; todos los datos de resultado deben escribirse en una matriz que se pasa a la función (si calcula un escalar, probablemente pasará una matriz de un elemento).\n",
        "2.   En una función de tipo **Kernel** se declara explícitamente su jerarquía de hilos cuando se les llama: es decir, el número de bloques de hilos y el número de hilos por bloque (hay que tener en cuenta que, si bien un *Kernel* se compila una vez, se puede llamar varias veces con diferentes tamaños de bloque o tamaños de grid).\n",
        "\n",
        "Nota (1): Los dispositivos *CUDA* más nuevos admiten el lanzamiento del *Kernel* del lado del dispositivo; esta característica se llama paralelismo dinámico.\n",
        "\n",
        "A primera vista, escribir un función tipo *Kernel* *CUDA* con *Numba* se parece mucho a escribir una función *JIT* para la *CPU*:\n",
        "\n",
        "El decorador `@cuda.jit`, le indica al interprete de *Python* que esta función debe **ejecutarse en la GPU** y por lo tanto su ejecución sera mucho más rapida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "557D42LNyAWd"
      },
      "source": [
        "# Ejemplos\n",
        "\n",
        "En esta sección vamos a ver algunos ejemplos de *Numba* accediendo a prestaciones de *CUDA*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_5x0HMMyRMc"
      },
      "source": [
        "## Suma de vectores (*CPU v.s. GPU*)\n",
        "\n",
        "Aunque la suma de vectores es un algoritmo sencillo de comprender y de implementar, es un excelente ejemplo para mostrar varios conceptos de *CUDA* y a su vez de *Numba*.\n",
        "\n",
        "Veamos la versión secuencial (*CPU*) de la suma de vectores en *Python*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlkQXSdky2Yn"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# suma de vectores secuencial\n",
        "def suma_vec_sec(arrRes, arrA, arrB):\n",
        "    # suma de cada entrada de ambos vectores\n",
        "    for pos in range(len(arrRes)):\n",
        "        arrRes[pos] = arrA[pos] + arrB[pos]\n",
        "\n",
        "# vectores a sumar\n",
        "a = np.array([1, 2, 3, 4, 5])\n",
        "b = np.array([10, 20, 30, 40, 50])\n",
        "c = np.array([0, 0, 0, 0, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbZnit8AzazZ"
      },
      "source": [
        "Es importante notar que esta versión secuencial hace uso del \"paso por referencia\" ya que el resultado de las operaciones se almacenan en uno de los parámetros que se reciben en la función `suma_vec_sec`.\n",
        "\n",
        "*Python* trabaja mediante *paso por valor* cuando se hace uso de valores primitivos (*int, float, string, etc.*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3hMam-EzQYc",
        "outputId": "a5c582de-b8bb-4603-e172-332c19e654ea"
      },
      "source": [
        "%%time\n",
        "# lanzamiento del kernel\n",
        "suma_vec_sec(c, b, a)\n",
        "\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11 22 33 44 55]\n",
            "CPU times: user 321 µs, sys: 79 µs, total: 400 µs\n",
            "Wall time: 391 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htEx5G-v0eA9"
      },
      "source": [
        "Ahora podemos recordar la forma de implementar y ejecutar este algoritmo mediante *CUDA* puro.\n",
        "\n",
        "Si das *click* en el siguiente [enlace](https://colab.research.google.com/github/jugernaut/ProgramacionEnParalelo/blob/main/CUDA/02_Algoritmos_CUDA_SCP.ipynb#scrollTo=6HGthAMmxdLf&line) verás el código que se mostró en el tema de *CUDA*.\n",
        "\n",
        "Puedes notar que son **más o menos 50 líneas de código**, sin contar el proceso de compilación y ejecución.\n",
        "\n",
        "Veamos ahora la versión en paralelo de este algoritmo, pero haciando uso de *Numba*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8WVrsgH5nvf"
      },
      "source": [
        "from numba import cuda\n",
        "import numpy as np\n",
        "\n",
        "# suma de vectores con CUDA mediante numba\n",
        "'''se puede definir el tipo de datos o dejarlo\n",
        "   a que lo infiera numba. Si se le indica a numba\n",
        "   el algoritmo se ejecuta un poco mas rapido\n",
        "   ¡¡¡PRUEBALO!!!'''\n",
        "#@cuda.jit('(int32[:], int32[:], int32[:])')\n",
        "@cuda.jit\n",
        "def suma_vec_par(arrRes, arrA, arrB):\n",
        "    # identificador del hilo en el bloque\n",
        "    tx = cuda.threadIdx.x\n",
        "    # identificador del bloque en el grid\n",
        "    ty = cuda.blockIdx.x\n",
        "    # dimension del bloque (cuantos hilos por bloque)\n",
        "    bw = cuda.blockDim.x\n",
        "    # POLINOMIO DE DIRECCIONAMIENTO\n",
        "    pos = tx + ty * bw\n",
        "    # limite de los vectores\n",
        "    if pos < arrRes.size:\n",
        "        arrRes[pos] = arrA[pos] + arrB[pos]\n",
        "\n",
        "# vectores a sumar\n",
        "a = np.array([1, 2, 3, 4, 5])\n",
        "b = np.array([10, 20, 30, 40, 50])\n",
        "c = np.array([0, 0, 0, 0, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzjSbmyYKl2f",
        "outputId": "4357c4e8-3612-44bd-a0c7-1e4e275ff850"
      },
      "source": [
        "%%time\n",
        "# parametros para lanzar el kernel\n",
        "hilos_por_bloque = len(a)+1//2\n",
        "bloques_por_grid = 2\n",
        "\n",
        "# lanzamiento del kernel\n",
        "suma_vec_par[hilos_por_bloque, bloques_por_grid](c, b, a)\n",
        "\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11 22 33 44 55]\n",
            "CPU times: user 162 ms, sys: 6.91 ms, total: 169 ms\n",
            "Wall time: 172 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 5 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diMNYgPlyRpk"
      },
      "source": [
        "## Lanzamiento de *Kernels*\n",
        "\n",
        "Cuando se ejecuta (o lanza) una función de tipo *Kernel*, se tiene que indicar el número de [hilos](https://colab.research.google.com/github/jugernaut/ProgramacionEnParalelo/blob/main/CUDA/01_CUDA_SCP.ipynb#scrollTo=c9kPwGmoNPbY&line) (procesos ligeros a nivel *GPU*) que se van a encargar de ejecutar este *Kernel* en paralelo.\n",
        "\n",
        "Existen varias formas de lanzar un *Kernel* en *Numba*.\n",
        "\n",
        "En la celda superior se hace uso de una de las multiples formas de invocar una función de tipo *Kernel*.\n",
        "\n",
        "Es importante notar que una vez definido el *Kernel* este **no se modifica aunque se invoque con diferente número de hilos**. Ahora veamos otras formas de invocar a este *Kernel*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYf31YOgK0I8"
      },
      "source": [
        "Forma 2 de lanzar el *Kernel*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_KU4CFEK6T0",
        "outputId": "cc57b57a-eb90-4f9b-8717-b9d0e0b6dab8"
      },
      "source": [
        "%%time\n",
        "# parametros para lanzar el kernel\n",
        "# dimension del grid (matriz)\n",
        "griddim = 1, 2\n",
        "# tamano del bloque de ejecucion\n",
        "blockdim = 1, 3\n",
        "\n",
        "# lanzamiento del kernel\n",
        "suma_vec_par[griddim, blockdim](c, b, a)\n",
        "\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11 22 33 44 55]\n",
            "CPU times: user 3.76 ms, sys: 0 ns, total: 3.76 ms\n",
            "Wall time: 3.78 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 2 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvl1eyOWzriv"
      },
      "source": [
        "`griddim` es el número de bloques de hilos por grid. Puede ser:\n",
        "\n",
        "*   Un entero.\n",
        "*   Una 1-tupla de enteros.\n",
        "*   Una 2-tupla de enteros.\n",
        "\n",
        "`blockdim` es el número de hilos por bloque. Puede ser:\n",
        "\n",
        "*   Un entero.\n",
        "*   Una 1-tupla de enteros.\n",
        "*   Una 2-tupla de enteros.\n",
        "*   Una 3-tupla de enteros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTdEkegILz6D"
      },
      "source": [
        "Forma 3 de lanzar un *Kernel*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne0GVoZiL4kd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb79dc6-ceb1-4730-b2bc-7ea4c28d93c3"
      },
      "source": [
        "%%time\n",
        "# parametros para lanzar el kernel\n",
        "hilos_por_bloque = 3\n",
        "blocks_por_grid = (c.size + (hilos_por_bloque - 1)) // hilos_por_bloque\n",
        "\n",
        "# lanzamiento del kernel\n",
        "suma_vec_par[blocks_por_grid, hilos_por_bloque](c, b, a)\n",
        "\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11 22 33 44 55]\n",
            "CPU times: user 3.05 ms, sys: 0 ns, total: 3.05 ms\n",
            "Wall time: 3.47 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLXcbpoM12b-"
      },
      "source": [
        "Hay que notar dos cosas:\n",
        "\n",
        "*   Cree una instancia del *Kernel* propiamente dicho, especificando un número de bloques (o \"bloques por grid\") y un número de hilos por bloque. El producto de los dos dará el número total de hilos lanzados. La instanciación del *Kernel* se realiza tomando la función del *Kernel* compilada (aquí `suma_vec_par`) e indexándola con una tupla de enteros.\n",
        "*   Ejecutando el *Kernel*, pasándole la matriz de entrada (y cualquier matriz de salida separada si es necesario). Por defecto, ejecutar un *Kernel* es sincrónico: la función regresa cuando el *Kernel* ha terminado de ejecutarse y los datos se vuelven a sincronizar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCKCamY69QK6"
      },
      "source": [
        "## Argumentos y funciones de tipo *device*\n",
        "\n",
        "Recordemos 2 puntos importantes de *CUDA* que se reflejan en *Numba*.\n",
        "\n",
        "1.   Una funcion de tipo *Kernel* (se ejecuta en el *GPU* pero se manda a llamar desde la *CPU*) no devuelve un valor, ya que los resultados se almacenan en alguno de los parámetros (paso por referencia), tal como se muestra en la siguiente celda.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A-W6bWG4gIY"
      },
      "source": [
        "# firma del decorador que indica tipo de los argumentos y valor de retorno\n",
        "@cuda.jit('void(int32[:], int32[:])')\n",
        "def foo(aryA, aryB):\n",
        "    \"Se procesa la informacion\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z2aGgcA4oIg"
      },
      "source": [
        "2.   Una función de tipo *device* se manda llamar y se ejecuta directamente en la *GPU*, este tipo de funciones pueden devolver valores dentro de la *GPU*, tal como se muestra en la celda inferior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwH-Gtne5P3t"
      },
      "source": [
        "'''firma del decorador que indica el tipo de los argumentos y el\n",
        "   valor de retorno, asi como el tipo de funcion (device)\n",
        "'''\n",
        "@cuda.jit('int32(int32, int32)', device=True)\n",
        "def bar(a, b):\n",
        "    \"Se realiza la suma de los vectores\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfSEIO685V0n"
      },
      "source": [
        "En la función anterior, le decimos al programa que ingresamos dos datos de tipo entero (`int32 , int32`) y extraemos otro dato de tipo entero (`int32`)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EaH9ZCFD027q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g5DB0rO503r"
      },
      "source": [
        "##Tamaño del bloque\n",
        "\n",
        "Puede parecer curioso tener una jerarquía de dos niveles al declarar el número de hilos que necesita un *Kernel*. El tamaño del bloque (es decir, el número de subprocesos por bloque) suele ser crucial:\n",
        "\n",
        "*   En el lado del software, el tamaño del bloque determina cuántos subprocesos comparten un área determinada de memoria compartida.\n",
        "*   En el lado del hardware, el tamaño del bloque debe ser lo suficientemente grande para la ocupación completa de las unidades de ejecución; Las recomendaciones se pueden encontrar en la Guía de programación *CUDA C/C++*.\n",
        "\n",
        "Para ayudar a lidiar con matrices multidimensionales, *CUDA* le permite especificar bloques y cuadrículas multidimensionales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gcrIb707Jmo"
      },
      "source": [
        "## Posición del hilo (subproceso)\n",
        "\n",
        "Cuando se ejecuta un *Kernel*, el código de la función del *Kernel* es ejecutado por cada hilo una vez. Por lo tanto, tiene que saber en qué hilo se encuentra, para saber de qué elemento (s) de la matriz es responsable (los algoritmos complejos pueden definir responsabilidades más complejas, pero el principio subyacente es el mismo).\n",
        "\n",
        "Una forma es que el hilo determine su posición en la grid y en el bloque y calcule manualmente la posición de la matriz correspondiente:\n",
        "\n",
        "Para un 1D grid:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRXXD4--70Q_",
        "outputId": "338056c3-6cb2-4a1a-e364-e15f2cad7150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "tx = cuda.threadIdx.x\n",
        "bx = cuda.blockIdx.x\n",
        "bw = cuda.blockDim.x\n",
        "i = tx + bx * bw\n",
        "array[i] = something(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-dce9fd9af142>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblockIdx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblockDim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msomething\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'property' and 'property'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrR9LgKH72zy"
      },
      "source": [
        "Para un 2D grid:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm9949TF76so",
        "outputId": "3e388d0e-f51c-469b-9408-0dfe5f34d775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "tx = cuda.threadIdx.x\n",
        "ty = cuda.threadIdx.y\n",
        "bx = cuda.blockIdx.x\n",
        "by = cuda.blockIdx.y\n",
        "bw = cuda.blockDim.x\n",
        "bh = cuda.blockDim.y\n",
        "x = tx + bx * bw\n",
        "y = ty + by * bh\n",
        "array[x, y] = something(x, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-c8454f5e4601>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblockDim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblockDim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mty\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msomething\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'property' and 'property'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvXx9Z5u8P_E"
      },
      "source": [
        "\n",
        "`threadIdx`, `blockIdx`, `blockDim` y `gridDim` son objetos especiales proporcionados por el *backend CUDA* con el único propósito de conocer la geometría de la jerarquía de hilos y la posición del hilo actual dentro de esa geometría.\n",
        "\n",
        "Estos objetos pueden ser 1D, 2D o 3D, dependiendo de cómo se invocó el *Kernel*. Para acceder al valor en cada dimensión, use los atributos $x, y$ y $z$ de estos objetos, respectivamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7xlhFvM8cfy"
      },
      "source": [
        "### `numba.cuda.threadIdx`\n",
        "\n",
        "Indíca el indice del hilo en el bloque actual. Para bloques 1D, el índice (dado por el atributo x) es un número entero que abarca el rango de 0 inclusive a `numba.cuda.blockDim` exclusivo. Existe una regla similar para cada dimensión cuando se utiliza más de una dimensión."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejIu_ve39U6Z"
      },
      "source": [
        "### `numba.cuda.blockDim`\n",
        "\n",
        "La \"forma\" (dimensión) del bloque de hilos, como se declaró al crear una instancia del *Kernel*. Este valor es el mismo para todos los hilos en un *Kernel* dado, incluso si pertenecen a bloques diferentes (es decir, cada bloque está \"lleno\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhBwHkze9krp"
      },
      "source": [
        "### `numba.cuda.blockIdx`\n",
        "\n",
        "Muestra el indice del bloque en la cuadrícula de hilos que lanzaron un *Kernel*. Para una cuadrícula 1D, el índice (dado por el atributo x) es un número entero que abarca el rango de 0 a `numba.cuda.gridDim` exclusivo. Existe una regla similar para cada dimensión cuando se utiliza más de una dimensión."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_TNrl5F9xXq"
      },
      "source": [
        "### `numba.cuda.gridDim`\n",
        "\n",
        "La forma de la grid de bloques, es decir, el número total de bloques lanzados por esta invocación del *Kernel*, como se declaró al crear una instancia del *Kernel*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTK26zP6_M2w"
      },
      "source": [
        "## Posición del hilo (subproceso) de manera compacta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPQwfyZs-EUU"
      },
      "source": [
        "### `numba.cuda.grid(ndim)`\n",
        "\n",
        "Devuelve la posición absoluta del hilo actual en toda la cuadrícula de `blocks.ndim` debe corresponder al número de dimensiones declaradas al crear una instancia del *Kernel*.\n",
        "\n",
        "Si `ndim` es 1, se devuelve un solo entero. Si `ndim` es 2 o 3, se devuelve una tupla del número dado de enteros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkPGyn81-qqY"
      },
      "source": [
        "\n",
        "### `numba.cuda.gridsize(ndim)`\n",
        "Devuelve el tamaño absoluto (o forma) en hilos de toda la cuadrícula de `blocks.ndim` tiene el mismo significado que en `grid()` anterior.\n",
        "\n",
        "Recordando que para un 1D grid se tenia el siguiente código:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl8y5BxW_-7e"
      },
      "source": [
        "tx = cuda.threadIdx.x\n",
        "bx = cuda.blockIdx.x\n",
        "bw = cuda.blockDim.x\n",
        "i = tx + bx * bw\n",
        "array[i] = something(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M74xPSzN__u_"
      },
      "source": [
        "Con la notación compacta de `cuda.grid` queda de la siguiente manera.\n",
        "\n",
        "Para un 1D grid:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8VbR0YGAKGu"
      },
      "source": [
        "i = cuda.grid(1)\n",
        "array[i] = something(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_XfXfBtAW9S"
      },
      "source": [
        "Recordando que para un 2D-grid se tenia el siguiente código:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbGAHJEQAbWx"
      },
      "source": [
        "tx = cuda.threadIdx.x\n",
        "ty = cuda.threadIdx.y\n",
        "bx = cuda.blockIdx.x\n",
        "by = cuda.blockIdx.y\n",
        "bw = cuda.blockDim.x\n",
        "bh = cuda.blockDim.y\n",
        "x = tx + bx * bw\n",
        "y = ty + by * bh\n",
        "array[x, y] = something(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSe4ghJlALC2"
      },
      "source": [
        "Con la notación compacta de `cuda.grid` queda de la siguiente manera.\n",
        "\n",
        "Para un 2D-grid:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkvZmUJdAOJt"
      },
      "source": [
        "x, y = cuda.grid(2)\n",
        "array[x, y] = something(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPdowzRAAlTS"
      },
      "source": [
        "Una implementación la incrementar en 1 los elementos de un vector (`arreglo`), con la notación larga, sería la siguiente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WflSehnAq6o"
      },
      "source": [
        "@cuda.jit\n",
        "def incrementeo_en_uno(un_vector):\n",
        "    # identificar del hilo en un bloque\n",
        "    tx = cuda.threadIdx.x\n",
        "    # identificador del bloque\n",
        "    ty = cuda.blockIdx.x\n",
        "    # ancho del bloque (cuantos hilos por bloque)\n",
        "    bw = cuda.blockDim.x\n",
        "    # POLINOMIO DE DIRECCIONAMIENTO\n",
        "    pos = tx + ty * bw\n",
        "    # revisamos limites del arreglo (OUT OF BOUNDS ERROR)\n",
        "    if pos < un_vector.size:\n",
        "        un_vector[pos] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-aUW23pAs7i"
      },
      "source": [
        "Con la notación compacta de `cuda.grid` queda de la siguiente manera.\n",
        "\n",
        "Para un 1D-grid (vector):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSvWiV21AxYD"
      },
      "source": [
        "@cuda.jit\n",
        "def incrementeo_en_uno(un_vector):\n",
        "    pos = cuda.grid(1)\n",
        "    if pos < an_array.size:\n",
        "        un_vector[pos] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftcUuSuxwrvT"
      },
      "source": [
        "# Ejemplos (vectores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYLVquykwvmO"
      },
      "source": [
        "## Código 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z00kS1jPwx1V",
        "outputId": "555a30c3-dba0-4fbe-f91c-b3f222103422"
      },
      "source": [
        "from numba import cuda\n",
        "import numpy as np\n",
        "@cuda.jit\n",
        "def increment_by_one(an_array):\n",
        "    # Thread id in a 1D block\n",
        "    tx = cuda.threadIdx.x\n",
        "    # Block id in a 1D grid\n",
        "    ty = cuda.blockIdx.x\n",
        "    # Block width, i.e. number of threads per block\n",
        "    bw = cuda.blockDim.x\n",
        "    # Compute flattened index inside the array\n",
        "    pos = tx + ty * bw\n",
        "    if pos < an_array.size:  # Check array boundaries\n",
        "        an_array[pos] += 1\n",
        "\n",
        "n = 32\n",
        "x = np.arange(n).astype(np.float32)\n",
        "threads_per_block=32\n",
        "blocks_per_grid=1\n",
        "print(\"Antes de lanzar el Kernel\")\n",
        "print(x)\n",
        "increment_by_one[threads_per_block,blocks_per_grid](x)\n",
        "print(\"Despues de lanzar el Kernel\")\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antes de lanzar el Kernel\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31.]\n",
            "Despues de lanzar el Kernel\n",
            "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
            " 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 32 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGOEFsAnykRN"
      },
      "source": [
        "## Código 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmsyPNP4ymbR",
        "outputId": "253c9930-cfe1-47cc-9232-44e82229b9bb"
      },
      "source": [
        "from numba import cuda\n",
        "import numpy as np\n",
        "@cuda.jit\n",
        "def increment_by_one(an_array):\n",
        "  pos=cuda.grid(1)\n",
        "  if pos < an_array.size:  # Check array boundaries\n",
        "    an_array[pos] += 1\n",
        "\n",
        "n = 32\n",
        "x = np.arange(n).astype(np.float32)\n",
        "threads_per_block=32\n",
        "blocks_per_grid=1\n",
        "print(\"Antes de lanzar el Kernel\")\n",
        "print(x)\n",
        "increment_by_one[threads_per_block,blocks_per_grid](x)\n",
        "print(\"Despues de lanzar el Kernel\")\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antes de lanzar el Kernel\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31.]\n",
            "Despues de lanzar el Kernel\n",
            "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
            " 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 32 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xncD626fzyA5"
      },
      "source": [
        "## Código 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItWrHwXRzzzm",
        "outputId": "83965a32-037d-455c-e465-3010bdd8bda7"
      },
      "source": [
        "from __future__ import division\n",
        "from numba import cuda\n",
        "import numpy\n",
        "import math\n",
        "\n",
        "# CUDA kernel\n",
        "@cuda.jit('void(float32[:])')\n",
        "def my_kernel(io_array):\n",
        "    pos = cuda.grid(1)\n",
        "    if pos < io_array.size:\n",
        "        io_array[pos] *= 2 # do the computation\n",
        "\n",
        "# Host code\n",
        "data = numpy.ones(256).astype(np.float32)\n",
        "threadsperblock = 256\n",
        "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
        "my_kernel[blockspergrid, threadsperblock](data)\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejiV7EXB4xAH"
      },
      "source": [
        "## Código 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jVyQ_qV2rdf",
        "outputId": "8bc78896-6c77-4536-fad2-80039bcf6a0e"
      },
      "source": [
        "from numba import cuda\n",
        "import numpy as np\n",
        "import math\n",
        "@cuda.jit\n",
        "def sum_arrays(x_in,y_in):\n",
        "  tId=cuda.threadIdx.x\n",
        "  bId=cuda.blockIdx.x\n",
        "  DimBloc=cuda.blockDim.x\n",
        "\n",
        "  pos=tId+bId*DimBloc\n",
        "\n",
        "  if pos<x_in.size:\n",
        "    x_in[pos]=x_in[pos]+y_in[pos]\n",
        "\n",
        "n = 32\n",
        "x = np.arange(n).astype(np.float32)\n",
        "y=np.arange(n).astype(np.float32)\n",
        "\n",
        "threads_per_block=32\n",
        "blocks_per_grid=math.ceil(x.size/threads_per_block)\n",
        "\n",
        "print(\"Antes de lanzar el Kernel\")\n",
        "print(x)\n",
        "\n",
        "sum_arrays[blocks_per_grid,threads_per_block](x,y)\n",
        "print(\"Despues de lanzar el Kernel\")\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antes de lanzar el Kernel\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31.]\n",
            "Despues de lanzar el Kernel\n",
            "[ 0.  2.  4.  6.  8. 10. 12. 14. 16. 18. 20. 22. 24. 26. 28. 30. 32. 34.\n",
            " 36. 38. 40. 42. 44. 46. 48. 50. 52. 54. 56. 58. 60. 62.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUxxiJSs4zT-"
      },
      "source": [
        "## Código 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWjyml2n40iu",
        "outputId": "43a8b75f-7bac-4d7d-a28f-8fc026b2c128"
      },
      "source": [
        "@cuda.jit\n",
        "def add_kernel(x, y, out):\n",
        "    tidx = cuda.threadIdx.x # this is the unique thread ID within a 1D block\n",
        "    bidx = cuda.blockIdx.x  # Similarly, this is the unique block ID within the 1D grid\n",
        "\n",
        "    block_dimx = cuda.blockDim.x  # number of threads per block\n",
        "    grid_dimx = cuda.gridDim.x    # number of blocks in the grid\n",
        "\n",
        "    start = tidx + bidx * block_dimx\n",
        "    stride = block_dimx * grid_dimx\n",
        "\n",
        "    # assuming x and y inputs are same length\n",
        "    for i in range(start, x.shape[0], stride):\n",
        "        out[i] = x[i] + y[i]\n",
        "\n",
        "n = 100000\n",
        "x = np.arange(n).astype(np.float32)\n",
        "y = 2 * x\n",
        "out = np.empty_like(x)\n",
        "\n",
        "threads_per_block = 128\n",
        "blocks_per_grid = 30\n",
        "\n",
        "%timeit add_kernel[blocks_per_grid, threads_per_block](x, y, out)\n",
        "print(out[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 30 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.94 ms ± 202 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
            "[ 0.  3.  6.  9. 12. 15. 18. 21. 24. 27.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk03c9ir4-p0",
        "outputId": "c9ffff29-232a-4b00-b44f-49ccb6f737e1"
      },
      "source": [
        "@cuda.jit\n",
        "def add_kernel(x, y, out):\n",
        "    tidx = cuda.threadIdx.x # this is the unique thread ID within a 1D block\n",
        "    bidx = cuda.blockIdx.x  # Similarly, this is the unique block ID within the 1D grid\n",
        "\n",
        "    block_dimx = cuda.blockDim.x  # number of threads per block\n",
        "    grid_dimx = cuda.gridDim.x    # number of blocks in the grid\n",
        "\n",
        "    start = tidx + bidx * block_dimx\n",
        "    stride = block_dimx * grid_dimx\n",
        "\n",
        "    # assuming x and y inputs are same length\n",
        "    if start<x.size:\n",
        "        out[start] = x[start] + y[start]\n",
        "\n",
        "n = 100000\n",
        "x = np.arange(n).astype(np.float32)\n",
        "y = 2 * x\n",
        "out = np.empty_like(x)\n",
        "\n",
        "threads_per_block = 128\n",
        "blocks_per_grid = 30\n",
        "\n",
        "%timeit add_kernel[blocks_per_grid, threads_per_block](x, y, out)\n",
        "print(out[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 30 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.13 ms ± 321 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
            "[ 0.  3.  6.  9. 12. 15. 18. 21. 24. 27.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSTeATMg5vx1",
        "outputId": "d970f42e-88a4-419b-9984-6f972b6b6237"
      },
      "source": [
        "@cuda.jit\n",
        "def add_kernel(x, y, out):\n",
        "    start = cuda.grid(1)\n",
        "\n",
        "    # assuming x and y inputs are same length\n",
        "    if start<x.size:\n",
        "        out[start] = x[start] + y[start]\n",
        "\n",
        "n = 100000\n",
        "x = np.arange(n).astype(np.float32)\n",
        "y = 2 * x\n",
        "out = np.empty_like(x)\n",
        "\n",
        "threads_per_block = 128\n",
        "blocks_per_grid = 30\n",
        "\n",
        "%timeit add_kernel[blocks_per_grid, threads_per_block](x, y, out)\n",
        "print(out[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 30 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.95 ms ± 121 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
            "[ 0.  3.  6.  9. 12. 15. 18. 21. 24. 27.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owEBlQg97T1V"
      },
      "source": [
        "# Ejemplos (matrices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HARp1HgA4fb"
      },
      "source": [
        "Bastante compacto y reducido en comparación con *CUDA*, ¿no?.\n",
        "\n",
        "Para un 2D-grid (matriz):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy4mcS1PA-oH"
      },
      "source": [
        "@cuda.jit\n",
        "def incremento_uno_matriz(an_array):\n",
        "    x, y = cuda.grid(2)\n",
        "    if x < an_array.shape[0] and y < an_array.shape[1]:\n",
        "       an_array[x, y] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IECWsmFcBK6a"
      },
      "source": [
        "Necesitamos definir la matriz con la que se va a trabajar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrXEBgehBSCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e407a09-42dd-4afc-c357-b77c03a0d1a2"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# matriz de 16x16 llena de unos\n",
        "matriz = np.ones((16, 16))\n",
        "\n",
        "# mostramos la matriz inicial\n",
        "print(matriz)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66YsJHgPQlzu"
      },
      "source": [
        "El lanzamiento de este *Kernel* podría ser de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_RKtLDhHZf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93790ae-176e-4a30-db10-a0b312a68253"
      },
      "source": [
        "%time\n",
        "hilos_por_bloque = (16, 16)\n",
        "# redondeamos para ajustarnos a los limites\n",
        "bloques_por_grid_x = math.ceil(matriz.shape[0] / hilos_por_bloque[0])\n",
        "bloques_por_grid_y = math.ceil(matriz.shape[1] / hilos_por_bloque[1])\n",
        "bloques_por_grid = (bloques_por_grid_x, bloques_por_grid_y)\n",
        "incremento_uno_matriz[bloques_por_grid, hilos_por_bloque](matriz)\n",
        "print(matriz)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.48 µs\n",
            "[[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQPsco8wIMCk"
      },
      "source": [
        "Mucho muy pequeño, compacto y comprensible el código escrito con *Numba*, de hecho con una mínima modificación en la definición de `incremento_uno_matriz` puedes contestar a la pregunta de este [notebook](https://colab.research.google.com/github/jugernaut/ProgramacionEnParalelo/blob/main/CUDA/02_Algoritmos_CUDA_SCP.ipynb#scrollTo=c9kPwGmoNPbY) respecto a las **operaciones con matrices** mediante *CUDA* o en este caso con *Numba*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cmelKtXHoUn"
      },
      "source": [
        "## Código 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9BcAcJV7MgF",
        "outputId": "b0be3981-6a05-4a9a-f101-b52d3ba6504d"
      },
      "source": [
        "@cuda.jit\n",
        "def increment_a_2D_array(an_array):\n",
        "    x, y = cuda.grid(2)\n",
        "    if x < an_array.shape[0] and y < an_array.shape[1]:\n",
        "       an_array[x, y] += 1\n",
        "\n",
        "n = 1024\n",
        "x = np.arange(n).astype(np.float32).reshape((32,32))\n",
        "\n",
        "threads_per_block = 16\n",
        "blockdim=threads_per_block , threads_per_block\n",
        "\n",
        "griddim=math.ceil(n/ blockdim[0]), math.ceil(n/ blockdim[1])\n",
        "\n",
        "increment_a_2D_array[griddim, blockdim](x)\n",
        "print(x[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n",
            "   15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n",
            "   29.  30.  31.  32.]\n",
            " [ 33.  34.  35.  36.  37.  38.  39.  40.  41.  42.  43.  44.  45.  46.\n",
            "   47.  48.  49.  50.  51.  52.  53.  54.  55.  56.  57.  58.  59.  60.\n",
            "   61.  62.  63.  64.]\n",
            " [ 65.  66.  67.  68.  69.  70.  71.  72.  73.  74.  75.  76.  77.  78.\n",
            "   79.  80.  81.  82.  83.  84.  85.  86.  87.  88.  89.  90.  91.  92.\n",
            "   93.  94.  95.  96.]\n",
            " [ 97.  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110.\n",
            "  111. 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124.\n",
            "  125. 126. 127. 128.]\n",
            " [129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140. 141. 142.\n",
            "  143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153. 154. 155. 156.\n",
            "  157. 158. 159. 160.]\n",
            " [161. 162. 163. 164. 165. 166. 167. 168. 169. 170. 171. 172. 173. 174.\n",
            "  175. 176. 177. 178. 179. 180. 181. 182. 183. 184. 185. 186. 187. 188.\n",
            "  189. 190. 191. 192.]\n",
            " [193. 194. 195. 196. 197. 198. 199. 200. 201. 202. 203. 204. 205. 206.\n",
            "  207. 208. 209. 210. 211. 212. 213. 214. 215. 216. 217. 218. 219. 220.\n",
            "  221. 222. 223. 224.]\n",
            " [225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238.\n",
            "  239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251. 252.\n",
            "  253. 254. 255. 256.]\n",
            " [257. 258. 259. 260. 261. 262. 263. 264. 265. 266. 267. 268. 269. 270.\n",
            "  271. 272. 273. 274. 275. 276. 277. 278. 279. 280. 281. 282. 283. 284.\n",
            "  285. 286. 287. 288.]\n",
            " [289. 290. 291. 292. 293. 294. 295. 296. 297. 298. 299. 300. 301. 302.\n",
            "  303. 304. 305. 306. 307. 308. 309. 310. 311. 312. 313. 314. 315. 316.\n",
            "  317. 318. 319. 320.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozHHmnQ0HsBV"
      },
      "source": [
        "## Código 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_EYoltl-JfP",
        "outputId": "76f78bac-e89d-43aa-d45e-4caa6d909e28"
      },
      "source": [
        "@cuda.jit\n",
        "def increment_a_2D_array(an_array,an_array1):\n",
        "    x, y = cuda.grid(2)\n",
        "    if x < an_array.shape[0] and y < an_array.shape[1]:\n",
        "       an_array[x, y] = an_array[x, y] + an_array1[x, y]\n",
        "\n",
        "n = 1024\n",
        "x = np.arange(n).astype(np.float32).reshape((32,32))\n",
        "y=np.arange(n).astype(np.float32).reshape((32,32))\n",
        "threads_per_block = 16\n",
        "blockdim=threads_per_block , threads_per_block\n",
        "\n",
        "griddim=math.ceil(n/ blockdim[0]), math.ceil(n/ blockdim[1])\n",
        "print(griddim)\n",
        "increment_a_2D_array[griddim, blockdim](x,y)\n",
        "print(x[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 64)\n",
            "[[  0.   2.   4.   6.   8.  10.  12.  14.  16.  18.  20.  22.  24.  26.\n",
            "   28.  30.  32.  34.  36.  38.  40.  42.  44.  46.  48.  50.  52.  54.\n",
            "   56.  58.  60.  62.]\n",
            " [ 64.  66.  68.  70.  72.  74.  76.  78.  80.  82.  84.  86.  88.  90.\n",
            "   92.  94.  96.  98. 100. 102. 104. 106. 108. 110. 112. 114. 116. 118.\n",
            "  120. 122. 124. 126.]\n",
            " [128. 130. 132. 134. 136. 138. 140. 142. 144. 146. 148. 150. 152. 154.\n",
            "  156. 158. 160. 162. 164. 166. 168. 170. 172. 174. 176. 178. 180. 182.\n",
            "  184. 186. 188. 190.]\n",
            " [192. 194. 196. 198. 200. 202. 204. 206. 208. 210. 212. 214. 216. 218.\n",
            "  220. 222. 224. 226. 228. 230. 232. 234. 236. 238. 240. 242. 244. 246.\n",
            "  248. 250. 252. 254.]\n",
            " [256. 258. 260. 262. 264. 266. 268. 270. 272. 274. 276. 278. 280. 282.\n",
            "  284. 286. 288. 290. 292. 294. 296. 298. 300. 302. 304. 306. 308. 310.\n",
            "  312. 314. 316. 318.]\n",
            " [320. 322. 324. 326. 328. 330. 332. 334. 336. 338. 340. 342. 344. 346.\n",
            "  348. 350. 352. 354. 356. 358. 360. 362. 364. 366. 368. 370. 372. 374.\n",
            "  376. 378. 380. 382.]\n",
            " [384. 386. 388. 390. 392. 394. 396. 398. 400. 402. 404. 406. 408. 410.\n",
            "  412. 414. 416. 418. 420. 422. 424. 426. 428. 430. 432. 434. 436. 438.\n",
            "  440. 442. 444. 446.]\n",
            " [448. 450. 452. 454. 456. 458. 460. 462. 464. 466. 468. 470. 472. 474.\n",
            "  476. 478. 480. 482. 484. 486. 488. 490. 492. 494. 496. 498. 500. 502.\n",
            "  504. 506. 508. 510.]\n",
            " [512. 514. 516. 518. 520. 522. 524. 526. 528. 530. 532. 534. 536. 538.\n",
            "  540. 542. 544. 546. 548. 550. 552. 554. 556. 558. 560. 562. 564. 566.\n",
            "  568. 570. 572. 574.]\n",
            " [576. 578. 580. 582. 584. 586. 588. 590. 592. 594. 596. 598. 600. 602.\n",
            "  604. 606. 608. 610. 612. 614. 616. 618. 620. 622. 624. 626. 628. 630.\n",
            "  632. 634. 636. 638.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfteTdToPWyV"
      },
      "source": [
        "# Glosario\n",
        "\n",
        "*Host*: En el contexto de *CUDA* el host es el *CPU* del dispositivo de cómputo en el que se ejecuta el algoritmo.\n",
        "\n",
        "Asíncrono: En computación un evento (proceso) asíncrono es aquel no tiene correspondencia temporal con otro evento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga6ceh7EifNN"
      },
      "source": [
        "# Referencias\n",
        "\n",
        "1.   https://numba.pydata.org/numba-doc/latest/user/5minguide.html\n",
        "2.   http://numba.pydata.org/numba-doc/latest/user/threading-layer.html\n",
        "3.   https://numba.pydata.org/numba-doc/dev/user/jit.html\n",
        "4.   https://thedatafrog.com/en/articles/make-python-fast-numba/\n",
        "5.   https://people.duke.edu/~ccc14/sta-663/CUDAPython.html\n",
        "6. Tolga Soyata: GPU Parallel Program Development Using Cuda.\n",
        "7. https://fisica.cab.cnea.gov.ar/gpgpu/images/clases/clase_1_cuda.pdf\n",
        "8. Dongarra Foster: Source Book of parallel computing.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}