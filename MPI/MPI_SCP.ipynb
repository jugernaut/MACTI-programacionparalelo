{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MPI_SCP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugernaut/ProgramacionEnParalelo/blob/main/MPI/MPI_SCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaqO9ukN90TX"
      },
      "source": [
        "<font color=\"Teal\" face=\"Comic Sans MS,arial\">\n",
        "  <h1 align=\"center\"><i>MPI (Intercambio de Mensajes).</i></h1>\n",
        "  </font>\n",
        "  <font color=\"Black\" face=\"Comic Sans MS,arial\">\n",
        "  <h5 align=\"center\"><i>Profesor: M.en.C. Miguel Angel Pérez León.</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Jesús Iván Coss Calderón.</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Mario Arturo .</i></h5>\n",
        "  <h5 align=\"center\"><i>Materia: Seminario de programación en paralelo..</i></h5>\n",
        "  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSSSzHnlC26v"
      },
      "source": [
        "#Introducción.\n",
        "\n",
        "Hoy en día existe una gran cantidad de lenguajes para realizar computo en paralelo. Muchos de ellos, lenguajes de alto nivel que facilitan (y ocultan) muchos de los aspectos de manejar computo en paralelo.\n",
        "\n",
        " \n",
        "\n",
        "Sin embargo hasta el momento no existe un solo lenguaje de alto nivel (por ejemplo java o python) que haya sido aceptado ampliamente por la comunidad del computo en paralelo.\n",
        "\n",
        "\n",
        "La mayor parte del computo en paralelo se realiza utilizando lenguajes como Fortran o C, con funciones aumentadas (MPI) que realizan el pase de mensajes entre procesos.\n",
        "\n",
        "\n",
        "MPI continua siendo el estándar mas popular para el modelo de programación en paralelo mediante pase de mensajes.\n",
        "\n",
        " \n",
        "Podríamos decir que la mayoría de las p.c's. actuales dan soporte para MPI y a su vez existen bibliotecas gratuitas que permiten realizar computo en paralelo.\n",
        "\n",
        "Al igual que OpenMP, MPI es un conjunto de bibliotecas, funciones y directivas de compilador (API) que permite programar en paralelo en conjunto con lenguajes como Fortran, C o C++.\n",
        "\n",
        "\n",
        "La principal característica de MPI es que este no se basa en un modelo de memoria compartida, por lo que la comunicación entre procesos se realiza mediante paso de mensajes.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/MPI/modelo.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/MPI/distributed_mem.gif?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/MPI/initfinal.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYctvrA4909M"
      },
      "source": [
        "#Desempeño. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfY5noetEVOA"
      },
      "source": [
        "##¿Como funciona?.\n",
        "\n",
        "La forma tradicional de la programación en paralelo empleando paso de mensajes es la siguiente:\n",
        "\n",
        "• Desde el momento en que se ejecuta un programa, se genera el número de procesos que llevaran a cabo el algoritmo.\n",
        "\n",
        "• Los procesos se comunican entre si enviando mensajes con la información necesaria.\n",
        "\n",
        "• Al termino de la o las secciones en paralelo, se junta el resultado del computo que hayan llevado a cabo los procesos.\n",
        "\n",
        "• Se devuelve un único resultado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzxML9tSElN7"
      },
      "source": [
        "##Ventajas. \n",
        "\n",
        "La principal diferencia entre MPI y OpenMPes que en este ultimo, se comienza y se termina con un solo hilo y en MPI desde que comienza el algoritmo se tienen varios procesos activos.\n",
        "\n",
        "\n",
        "Su principal ventaja es que no se requiere arquitectura de memoria compartida por lo que MPI puede ser empleando en casi cualquier sistema de computo en paralelo.\n",
        "\n",
        "\n",
        "Los elementos que intervienen cuando se emplea MPI son: el proceso que envía, el que recibe y el mensaje.\n",
        "\n",
        "\n",
        "Dependiendo de si el proceso que envía el mensaje requiere esperar o no, podemos pensar en que el paso de mensajes es de tipo síncrono o asíncrono.\n",
        "\n",
        "\n",
        "Dentro del paso de mensajes síncrono se engloba a las llamadas a procedimiento remoto, que son muy populares en las arquitecturas cliente/servidor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtPsRRfHE6e0"
      },
      "source": [
        "#MPI (API).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3tkBuVEFL1W"
      },
      "source": [
        "##Funciones importantes.\n",
        "\n",
        "Con MPI básicamente lo que tendremos son funciones y algunas de ellas son:\n",
        "\n",
        " \n",
        "\n",
        "*   MPI_Init: instrucción que indica que haremos uso de MPI, si ella no es posible utilizar MPI.\n",
        "*   MPI_Comm_rank: devuelve el identificador de un proceso.\n",
        "\n",
        "*   MPI_Comm_size: muestra el numero de procesos.\n",
        "*   MPI_Reduce: operación de reducción como en OpenMP.\n",
        "\n",
        "*   MPI_Finalize: desconecta las funciones de MPI.\n",
        "*   MPI_Barrier: función para sincronizar los procesos existentes.\n",
        "\n",
        "*   MPI_Wtime: devuelve el tiempo en el momento de ser llamada.\n",
        "*   MPI_Wtick: determina la precisión del timer.\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9h06jfLF5gy"
      },
      "source": [
        "#Compilación y Ejecución."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgkYa9znF9Er"
      },
      "source": [
        "##Distintas formas de identificarlo.\n",
        "\n",
        "De igual manera que OpenMP, una vez instalado el compilador y herramientas del lenguaje C (al menos en s.o. Linux) el API de MPI ya esta incluido con estas.\n",
        "\n",
        " \n",
        "\n",
        "De tal forma que compilar y ejecutar código que emplea directivas y funciones de MPI es tan sencillo como:\n",
        "\n",
        " \n",
        "\n",
        "Para compilar: \n",
        "\n",
        "\\$mpicc codigoc.c -o (salida.o) codigo\n",
        "\n",
        " \n",
        "\n",
        "El comando anterior compila (y en caso de no haber errores) y genera un archivo ejecutable (binario) que puede ser ejecutado de la siguiente manera:\n",
        "\n",
        " \n",
        "\n",
        "Para ejecutar, se puede usar la bandera -np para indicar el numero de procesos que se desean generar:$mpirun codigo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RouR47xeGEv1"
      },
      "source": [
        "#Glosario.\n",
        "\n",
        "Proceso: En el contexto de MPI un proceso, es un conjunto de instrucciones que son ejecutadas por el CPU.\n",
        "\n",
        "\n",
        "Asíncrono: En computación un evento (proceso) asíncrono es aquel no tiene correspondencia temporal con otro evento. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6A4TOBfGOQt"
      },
      "source": [
        "#Referencias.\n",
        "\n",
        "1. Michaell J. Quuin: Parallel Programming in C with OpenMP and MPI.\n",
        "\n",
        "2. https://lsi.ugr.es/jmantas/ppr/ayuda/mpi_ayuda.php\n",
        "\n",
        "3. Dongarra Foster: Source Book of parallel computing."
      ]
    }
  ]
}
