{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Envoltorios_SCP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugernaut/ProgramacionEnParalelo/blob/desarrollo/Envoltorios/01_Envoltorios_SCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCy4sQbk6TDS"
      },
      "source": [
        "<font color=\"Teal\" face=\"Comic Sans MS,arial\">\n",
        "  <h1 align=\"center\"><i>Envoltorios (Wrappers)</i></h1>\n",
        "  </font>\n",
        "  <font color=\"Black\" face=\"Comic Sans MS,arial\">\n",
        "  <h5 align=\"center\"><i>Profesor: M. en C. Miguel Angel Pérez León</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Jesús Iván Coss Calderón</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Mario Arturo Nieto Butron</i></h5>\n",
        "  <h5 align=\"center\"><i>Materia: Seminario de programación en paralelo</i></h5>\n",
        "  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsaIeAtF6T1G"
      },
      "source": [
        "# Introducción\n",
        "\n",
        "Una vez que ya se conocen las principales *API's* para programar en paralelo como *OpenMP*, *MPI* o *CUDA*, así como sus ventajas y desventajas podemos comenzar a utilizar alternativas como lo son los ***wrappers*** (envoltorios).\n",
        "\n",
        "Un *wrapper* es un conjunto de librerías y herramientas (en otro lenguaje diferente a *C/C++*) que actúa como puente y oculta muchos de los detalles de este tipo de *API's*.\n",
        "\n",
        "Existe una infinidad de lenguajes de alto nivel que permiten hacer uso de estos *wrappers*, como lo son *JAVA*, *Python*, *R*, etc.\n",
        "\n",
        "Para esta presentación nos enfocaremos en el lenguaje *Python* y algunos de los *wrappers* que existen en este lenguaje ya que las ventajas que ofrece este lenguaje lo hacen ideal para su uso en este curso.\n",
        "\n",
        "Dos de los envoltorios más populares para *Python* son *Numba* y *TensorFlow*.\n",
        "\n",
        "A pesar de la gran cantidad de *wrappers* que existen actualmente, debido a los alcances del curso, solo podremos revisar *Numba* y *TensorFlow*.\n",
        "\n",
        "Aquí podemos ver las diferentes capas que se construyen con *Python*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/wrapper.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Los 2 diferentes enfoques que se les puede dar a los envoltorios.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/arribabajo.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Flujo de *Numba*, ¿cómo es que se optimiza el código?.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/numba.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/numba-arch.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Relación entre *TensorFlow* y *Nvidia*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/tensor2.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/tensor1.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Capas en el desarrollo de *software*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/tensordevice.jpg?raw=1\" width=\"600\"> \n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PPn3oSp8WaM"
      },
      "source": [
        "# *Numba*\n",
        "\n",
        "*Numba* es uno de los envoltorios más conocidos y empleados actualmete debido a que su funcionamiento es muy sencillo así como su instalación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOiTvC5u8cQP"
      },
      "source": [
        "## ¿Cómo funciona?\n",
        "\n",
        "*Numba* tiene multiples formás de optimizar codigo y lograr que este muestre un mejor desempeño, esto lo realiza mediante alguna de las siguientes variantes:\n",
        "\n",
        "• Convierte código *Python* en código de máquina: al compilar código empleando *Numba*, este convierte el código en código de máquina y la segunda vez que sea ejecutado este se ejecuta en lenguaje de muy bajo nivel que se traduce en una ejecución más rápida.\n",
        "\n",
        "• Es posible utilizar una capa (*layer*) para acceder a características de *OpenMP*.\n",
        "\n",
        "• Es posible paralelizar código empleando utilidades de *MPI*.\n",
        "\n",
        "• Tiene soporte para el uso de *GPU's* utilizando *CUDA* como *background*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PehjbuHF8sKR"
      },
      "source": [
        "## Ventajas\n",
        "\n",
        "*Numba* posee múltiples ventajas, aunque una de las más importantes es poder decidir como optimizar el código escrito en *Python*.\n",
        "\n",
        "Es muy sencillo de instalar mediante *pip* e igual de fácil de usar que *Python*.\n",
        "\n",
        "Se tiene una gran capacidad de acoplamiento con *Numpy* (biblioteca para cómputo científico).\n",
        "\n",
        "Además de ser posible optar por un mecanismo para optimizar el código, *Numba* permite escribir código híbrido que combine lo mejor de las diferentes formás de optimizar el desempeño.\n",
        "\n",
        "Emplear *Numba* es tan sencillo como importar la biblioteca y hacer uso de sus **decoradores** para optimizar el código.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJhW1gUa9DdF"
      },
      "source": [
        "## Desventajas\n",
        "\n",
        "*Numba* tiene en realidad muy pocas desventajas.\n",
        "\n",
        "La más evidente de estas es que **encapsula mucho de su funcionamiento**, es decir que en realidad funciona como caja negra.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg_-5MXa94pG"
      },
      "source": [
        "### Decoradores\n",
        "\n",
        "Un decorador en *Numba* es una forma de modificar funciones de manera tal que pueda ser optimizada empleando alguna de las técnicas previamente mencionadas.\n",
        "\n",
        "Se puede pensar en un decorador en una función que recibe una función como parámetro y devuelve otra función optimizada como salida.\n",
        "\n",
        "Una función de *Python* es envuelta por uno o más decoradores, una vez que se define esta función el decorador es evaluado y *Numba* devuelve una función optimizada que puede ser invocada desde *Python*.\n",
        "\n",
        "El alcance del ó de los decoradores se limita al alcance de la función definida a la cual se le aplique dichos decoradores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY9qCM04-zEJ"
      },
      "source": [
        "### `nopython`\n",
        "\n",
        "Un decorador se utiliza mediante la sentencia `@jit(parametros)`.\n",
        "\n",
        "La forma más básica en la cual se puede usar *Numba*, es mediante el decorador `@jit(nopython=True)`.\n",
        "\n",
        "Esta sentencia lo que le indica a *Numba* es que el código en el cual esta envuelta la función, debe ser compilado y ejecutado sin utilizar el entono de *Python*. Lo que significa que una vez que ha sido compilada esta función se ejecutara de manera más eficiente que empleando el interprete de *Python*. \n",
        "\n",
        "Existe otro modo de compilación conocido como *object mode*, y se accede a este cuando no se hace uso del parámetro `nopython=True`, es decir `@jit` sin parámetros. Sin embargo este modo se limita a optimizar unicamente los ciclos y no todo el código definido en la función.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0LL39Jv-9l8"
      },
      "source": [
        "### `parallel`\n",
        "\n",
        "Otro parámetro muy útil pero a la vez 'obscuro' es, `@njit(parrallel=True)`.\n",
        "\n",
        "Este decorador va de la mano de la palabra reservada `prange` y en conjunto permiten ejecutar en paralelo ciclos dentro de la función definida.\n",
        "\n",
        "Este decorador oculta mucho del proceso que se realiza al ejecutar un algoritmo en paralelo. Sin embargo ya que a esta altura del curso se conoce cual es el transfondo (*OpenMP, MPI, CUDA*), podemos obviar el mismo.\n",
        "\n",
        "La parlabra reservada `prange` se emplea para especificar el ciclo que se quiere realizar en paralelo y no solo eso, también realiza la operación conocida como *reduction* de alguna variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uifxfic__Wcw"
      },
      "source": [
        "## *Numba* + *CUDA*\n",
        "\n",
        "*Numba* ofrece soporte para programación de *GPU* mediante *CUDA*, permitiendo compilar un subconjunto restringido de código escrito en *Python* que se traduce en funciones tipo *Kernel* y tipo *Device*.\n",
        "\n",
        "Una característica importante de *Numba*, es que al definir funciones de tipo *Kernel*, *Numba* hace parecer que esa función tiene acceso directo a arreglos de tipo *Numpy*. Los arreglos de tipo *Numpy* que se pasan como parámetro a las funciones de tipo kernel se transfieren de forma automática entre la memoria del *CPU* y del *GPU*, digamos que **oculta el proceso de *paso por referencia* y el reservar memoria con `malloc`**.\n",
        "\n",
        "*Numba* no tiene una implementación directa para todo el *API* de *CUDA*, de tal forma que algunas características de *CUDA* no son accesibles desde *Numba*. Sin embargo las funciones definidas en *Numba* son suficientes para comenzar a desarrollar algoritmos que hagan uso del o de los *GPU's* del dispositivo de cómputo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hiK7syt_wgx"
      },
      "source": [
        "### Declaración y uso de un *Kernel*\n",
        "\n",
        "La misma terminología empleada en el desarrollo de código usando *CUDA*, se aplica para el desarrollo de código mediante *Numba*.\n",
        "\n",
        "*   Una función de tipo *Kernel* no puede devolver un tipo de manera explicita; cualquier resultado de la función *kernel* debe ser almacenado en el arreglo de tipo *Numpy* que se pasa coma parámetro a esta función (paso por referencia).\n",
        "*   Cuando se ejecuta un *Kernel* se debe declarar de manera explícita la jerarquía de hilos, es decir; el número de bloques de hilos y el número de hilos por bloque.\n",
        "*   Es importante notar que un *Kernel* se compila una sola vez, pero puede ser llamado con diferentes tamaños de bloque o de *grid*.\n",
        "*   En caso de tener acceso a una tarjeta *Nvidia*, es posible emplear el simulador de [CUDA](https://nyu-cds.github.io/python-numba/05-cuda/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3WQ33GHAwOz"
      },
      "source": [
        "### Observaciones y Recomendaciones\n",
        "\n",
        "Por default la ejecución del *Kernel* se realiza de manera síncrona; la función termina cuando el *Kernel* ha terminado su ejecución y los datos son persistentes.\n",
        " \n",
        "Para elegir el tamaño del bloque, es decir el número de hilos por bloque hay que considerar 2 cosas: \n",
        "\n",
        "1.   Del lado del software: el tamaño del bloque determina **cuantos hilos comparten memoria**.\n",
        "2.   Del lado del hardware: el tamaño del bloque debe ser suficientemente grande para **ocupar todas las unidades de ejecución**.\n",
        "\n",
        "Sugerencias para identificar el tamaño del bloque pueden encontrarse en este [sitio](https://docs.nvidia.com/cuda/cuda-c-programming-guide/) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asKyxjQD9Pip"
      },
      "source": [
        "## Instalación en equipo local\n",
        "\n",
        "Dado que a partir de este punto del curso es posible emplear diferentes versiones de *Python* o de sus bibliotecas, se recomienda crear un **entorno virtual** y en este entorno instalar *numba*.\n",
        "\n",
        "Supongamos que ya se cuenta *virtualenv*, *Python* 2.7 y *pip*.\n",
        "\n",
        "1.   Crear entorno virtual: \n",
        "\n",
        "`\\$mkdir numba`\n",
        "\n",
        "`\\$virtualenv numba`\n",
        "\n",
        "2.   Activar entorno virtual:\n",
        "\n",
        "`\\$source numba/bin/activate`\n",
        "\n",
        "3.   Instalar *Numba*:\n",
        "\n",
        "`(numba)\\$pip install numba`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geIA0krQ2-VZ"
      },
      "source": [
        "## *Numba* en *Google Colab*\n",
        "\n",
        "Para utilizar *Numba* en *Google Colab*, es tan sencillo como importar la biblioteca y utilizar sus decoradores, a continuación se muestra el ejemplo de la aproximación de $pi$ optimizado mediante *Numba*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brKZ9XmT3VnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0190bf-f700-4dd2-8801-693c141303d2"
      },
      "source": [
        "import random\n",
        "from timeit import default_timer as timer\n",
        "# biblioteca de Numba\n",
        "from numba import jit\n",
        "\n",
        "#¡¡¡descomentar para optimizar, te vas a sorprender!!!\n",
        "#@jit(nopython=True)\n",
        "def mc_pi_aprox(n=100000000):\n",
        "    dentro_circulo = 0 \n",
        "    for i in range(n):\n",
        "      x = random.random()\n",
        "      y = random.random()\n",
        "      # valores dentro de la circunferencia\n",
        "      if (x**2+y**2 < 1):\n",
        "         dentro_circulo += 1\n",
        "    return 4*dentro_circulo / n\n",
        "\n",
        "# inicial\n",
        "inicio = timer()\n",
        "# algoritmo\n",
        "print(mc_pi_aprox())\n",
        "# final\n",
        "final = timer()\n",
        "\n",
        "# tiempo\n",
        "print('Tomo:',final - inicio, 'segundos')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.1415458\n",
            "Tomo: 45.764992242000005 segundos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stEwY6wIBNBZ"
      },
      "source": [
        "# *TensorFlow*\n",
        "\n",
        "*TensorFlow* es un conjunto de herramientas que proporciona *Google* para el desarrollo de algoritmos de aprendizaje automático, algunas de sus características son:\n",
        "\n",
        "*   Cuenta con diferentes versiones de su *API* para lenguajes tales como: *C++, Haskell, Java* y *Go* entre algunos, aunque la versión más usada es la de *Python*.\n",
        "*   Existen versiones optimizadas de *TensorFlow* que hacen uso de programación mediante *GPU's* o incluso mediante *TPU's*.\n",
        "*   Una de sus aplicaciones más comunes es en el desarrollo de **redes neuronales e inteligencia artificial**.\n",
        "*   El desarrollo de *TensorFlow* es mediante licencia de código abierto, lo que significa que no hace falta pagar una licencia para hacer uso del mismo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlEvJkJPBUtk"
      },
      "source": [
        "## Aplicaciones\n",
        "\n",
        "*TensorFlow* tiene bastas aplicaciones, algunas de ellas son:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   I.A. detrás de las fotografiás en *smarthphones*: recientemente se ha empleado técnicas de i.a. para mejorar la captura de imágenes en un *smartphone*, detrás de esta i.a podemos encontrar bibliotecas como *TensorfFlow*.\n",
        "*   Diagnostico médico: *TensorFlow* ya está mejorando las herramientas que utilizan los médicos, por ejemplo ayudando a analizar radiografías o fotografiás de pacientes y sugiriendo un diagnostico casi de inmediato.\n",
        "*   Procesamiento de imágenes: una de las aplicaciones más conocidas de *TensorFlow* es el *software* automatizado de procesamiento de imágenes, ***DeepDream*** es de los ejemplos mas conocidos al respecto. \n",
        "*   El desarrollo de *TensorFlow* es mediante licencia de código abierto, lo que significa que no hace falta pagar una licencia para hacer uso del mismo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX6YRj3mBavU"
      },
      "source": [
        "## ¿Cómo funciona?\n",
        "\n",
        "La idea de *TensorFlow 1.x* es definir el cómputo de datos como una gráfica conformada por **tensores** (nodos) y **datos** (aristas).\n",
        "\n",
        "*   **Grafo**: un grafo es un conjunto de nodos y aristas que representan el cómputo de información que se recibe como entrada.\n",
        "*   **Nodo**: es una agrupación de datos que puede tomar diferentes formas dependiendo del rango; rango 0 es un escalar, rango 1 es un vector, rango 2 una matriz, etc.\n",
        "*   **Tensor**: en el contexto de *TensorFlow*, un tensor es conocido como una operación (op), la cual recibe uno o más datos o incluso la salida de otro tensor y realiza la operación indicada.\n",
        "*   **Sesión**: para poder realizar los cálculos definidos en el grafo, se debe llevar a cabo mediante una sesión que representa el cálculo que se desea realizar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgF-TNuVBjec"
      },
      "source": [
        "##Visualización\n",
        "\n",
        "Podemos pensar en un grafo de la siguiente forma.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/op1.png?raw=1\" width=\"450\"> \n",
        "</center>\n",
        "\n",
        "Un perceptrón simple se ve así.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/op2.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Una capa de una red neuronal se ve de esta manera.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/opn.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Finalmente una red neuronal (*SOM*), la podemos pensar así.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/som.gif?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6qgBqlwCGJO"
      },
      "source": [
        "##  Instalación en equipo local\n",
        "\n",
        "Supongamos que ya se cuenta *virtualenv, python 2.7 y pip*.\n",
        "\n",
        "1.   Crear entorno virtual: \n",
        "\n",
        "`\\$mkdir tensorflow`\n",
        "\n",
        "`\\$virtualenv tensorflow`\n",
        "\n",
        "2.   Activar entorno virtual:\n",
        "\n",
        "`\\$source tensorflow/bin/activate`\n",
        "\n",
        "3.   Instalar *tensorflow*:\n",
        "\n",
        "`(tensorflow)\\$pip install tensorflow==1.0`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBIJYwmg48Zr"
      },
      "source": [
        "## *Tensorflow* 1.x en *Google Colab*\n",
        "\n",
        "En caso de usar la versión más reciente de *TensorFlow*, sucede igual que con *Numba*, solo es necesario importar la biblioteca y hacer uso de la misma, en otro caso es necesario desintalar la versión actual e instalar la versión necesaria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYYouZoXxuP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c84d6345-9be3-4a45-9be0-a361d1210076"
      },
      "source": [
        "# se le indica a python la version de tensorflow\n",
        "%tensorflow_version 1.x\n",
        "# importamos tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "# se genera una operacion\n",
        "a = tf.add(3, 5)\n",
        "# se muestran las caracteristicas de esta operacion\n",
        "print(a)\n",
        "\n",
        "# es solo hasta el momento de iniciar una sesion que se puede ver un resultado\n",
        "sess = tf.Session()\n",
        "# esta forma de definir el flujo de datos esta relacionado con ml\n",
        "print(sess.run(a))\n",
        "sess.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Tensor(\"Add:0\", shape=(), dtype=int32)\n",
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9fB90WK51MJ"
      },
      "source": [
        "# Otros envoltorios\n",
        "\n",
        "Existe una gran variedad de envoltorios para multiples lenguajes, por ejemplo la clase *Thread* o la interfaz *Runnable* del lenguaje *Java*.\n",
        "\n",
        "Un ejemplo de lo que se puede realizar con este tipo de envoltorios es el entorno de [*NetLogo*](http://www.netlogoweb.org/launch#http://www.netlogoweb.org/assets/modelslib/Sample%20Models/Art/Follower.nlogo), herramienta/lenguaje de simulación de modelos basados en agentes.\n",
        "\n",
        "En *Python* existe una cantidad enorme de envoltorios como: *PyThorch, Keras, PyCuda*, etc. De igual manera para otros lenguajes existen sus respectivas versiones de envoltorios que tienen sus fundamentos en *OpenMP, MPI* y *CUDA*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dsaFmbyCamn"
      },
      "source": [
        "# Glosario\n",
        "\n",
        "*Layer*: Capa informática, nivel o capa que se oculta una parte del *software*.\n",
        "\n",
        "*Background*: En computación entorno que da soporte a un determinado software. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J_aD2oaChF-"
      },
      "source": [
        "# Referencias\n",
        "\n",
        "1. https://numba.pydata.org/numba-doc/latest/user/5minguide.html\n",
        "\n",
        "2. https://nyu-cds.github.io/python-numba/01-jit/\n",
        "\n",
        "3. https://christophdeil.com/download/2019-07-11_Christoph_Deil_Numba.pdf\n",
        "\n",
        "4. https://numba.pydata.org/numba-doc/dev/cuda/kernels.html\n",
        "\n",
        "5. Tolga Soyata:\\newblock GPU Parallel Program Development Using CUDA."
      ]
    }
  ]
}