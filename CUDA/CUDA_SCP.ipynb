{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUDA_SCP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugernaut/ProgramacionEnParalelo/blob/main/CUDA/CUDA_SCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rYgUDA8Ltm4"
      },
      "source": [
        "<font color=\"Teal\" face=\"Comic Sans MS,arial\">\n",
        "  <h1 align=\"center\"><i>Cuda (Programación con GPU's).</i></h1>\n",
        "  </font>\n",
        "  <font color=\"Black\" face=\"Comic Sans MS,arial\">\n",
        "  <h5 align=\"center\"><i>Profesor: M.en.C. Miguel Angel Pérez León.</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Jesús Iván Coss Calderón.</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Mario Arturo .</i></h5>\n",
        "  <h5 align=\"center\"><i>Materia: Seminario de programación en paralelo..</i></h5>\n",
        "  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTIY01-ELvbH"
      },
      "source": [
        "#Introducción.\n",
        "\n",
        "Ademas de las herramientas previamente vistas para programación en paralelo (OpenMP y MPI), otra forma de realizar cómputo en paralelo es mediante Cuda.\n",
        "\n",
        " \n",
        "\n",
        "Cuda debe su nombre al acrónimo en ingles de Arquitectura Unificada de Dispositivos de Computo (Compute Unified of Device Arquitecture).\n",
        "\n",
        " \n",
        "\n",
        "De manera similar a OpenMP y MPI, Cuda es un API que en conjunto con el lenguaje C, permite realizar cómputo en paralelo empleando los GPU's que se tengan disponibles.\n",
        "\n",
        " \n",
        "\n",
        "Existen diferentes wrappers (envoltorios) para utilizar Cuda en otros lenguajes como Python, Fortran o incluso Java, sin embargo en esta presentación veremos las instrucciones básicas para el lenguaje C.\n",
        "\n",
        " \n",
        "\n",
        "Vale la pena mencionar que el desarrollo de este API (Cuda) es llevado a cabo por la empresa Nvidia.\n",
        "\n",
        "Inicialmente los GPU's (unidades de procesamiento gráfico) fueron diseñadas para procesamiento de imágenes, sin embargo se ha probado que muestran un gran desempeño no solo en esta área.\n",
        "\n",
        " \n",
        "\n",
        "La forma en la que trabaja Cuda es enviando la información a procesar directamente a la tarjeta gráfica, ahí es procesada y finalmente devuelta al CPU.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/gpuwork.jpg?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/gpucpu.jpg?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/menycore.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/cuda.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyAG-V_3MwrN"
      },
      "source": [
        "#Desempeño.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6YKyaVxM19I"
      },
      "source": [
        "##¿Como funciona?.\n",
        "\n",
        "La forma tradicional de la programación en paralelo empleando Cuda es la siguiente:\n",
        "\n",
        "• En un determinado momento en que se ejecuta un programa, se envía desde la memoria RAM la información que sera procesada por los GPU's.\n",
        "\n",
        "• Una vez que la información se ubica en la tarjeta gráfica se generan los thread (a nivel GPU) que se vayan a ocupar.\n",
        "\n",
        "• Se realizan los cálculos necesarios en paralelo.\n",
        "\n",
        "• Se devuelve un el resultado del procesamiento al CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A57hiA_xM7DU"
      },
      "source": [
        "##Ventajas.\n",
        "\n",
        "La principal diferencia entre MPI, OpenMP y Cuda, es que este ultimo realiza todos los cálculos empleando GPU's en lugar de los CPU's que utilizan MPI y OpenMP.\n",
        "\n",
        " \n",
        "\n",
        "La ventaja mas evidente de Cuda es la de utilizar GPU's, mismos que están optimizados para operaciones matriciales y vectoriales.\n",
        "\n",
        " \n",
        "\n",
        "La memoria constante, como su propio nombre indica, se usa para albergar datos que no cambian durante el transcurso de ejecución de un kernel, el uso de memoria constante en lugar de la memoria global pueda reducir considerablemente el tiempo de procesamiento.\n",
        "\n",
        " \n",
        "\n",
        "Se tiene soporte a nivel hardware para operaciones con enteros y con bits.\n",
        "\n",
        " \n",
        "\n",
        "Cuda ha mostrado gran desempeño para labores de inteligencia artificial tales como deep learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4MYFdgKNDcB"
      },
      "source": [
        "##Desventajas.\n",
        "\n",
        "No todo es miel sobre hojuelas al emplear Cuda y aquí algunas de sus desventajas.\n",
        "\n",
        " \n",
        "\n",
        "La transferencia de información entre el CPU y la tarjeta gráfica genera un cuello de botella, revisar ley de Amdhal\n",
        "\n",
        " \n",
        "\n",
        "No es posible ejecutar algoritmos recursivos.\n",
        "\n",
        " \n",
        "\n",
        "La unidad mínima de bloques debe ser de 32 threads.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WogAVZ_eNKKm"
      },
      "source": [
        "#Cuda."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9kPwGmoNPbY"
      },
      "source": [
        "##Funcionamiento de Cuda.\n",
        "\n",
        "A diferencia de los API's anteriores (OmpenMP, MPI) Cuda se maneja un poco diferente.\n",
        "\n",
        " \n",
        "\n",
        "En Cuda toda sección de código en paralelo se ejecuta mediante un Kernel, es decir Kernel = Función.\n",
        "\n",
        " \n",
        "\n",
        "Un Kernel se ejecuta en paralelo mediante Threads de GPU.\n",
        "\n",
        " \n",
        "\n",
        "Cada Kernel esta dividido en Blocks de una, dos o tres dimensiones de Threads.\n",
        "\n",
        " \n",
        "\n",
        "Finalmente un Grid puede ser una arreglo de una, dos o tres dimensiones de Blocks.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/thread.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/bloque.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/grid.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/total.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6Z7lCWUN7kY"
      },
      "source": [
        "#Jerarquía de memoria.\n",
        "\n",
        "Dadas los elementos del funcionamiento de Cuda, se tienen diferentes tipos de memoria y acceso a la misma, veamos.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/memlocal.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/membloque.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/memglobal.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/memtotal.png?raw=1\" width=\"600\"> \n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UGHISwQOkgt"
      },
      "source": [
        "#API Cuda.\n",
        "\n",
        "Los identificadores de funciones mas importantes que se tienen en Cuda son los siguientes.\n",
        "\n",
        " \n",
        "\n",
        "1. __global__: indica que una función es de tipo Kernel, es decir que sera ejecutada en un dispositivo (GPU) y solo puede ser llamada desde el host. Al momento de ser invocada se genera un grid de bloques con un número fijo e igual de threads.\n",
        "\n",
        " \n",
        "\n",
        "2. __device__: es una función que solo puede ser llamada desde un dispositivo, es decir que esta función solo puede ser invocada desde un kernel o desde otra función de tipo device. \n",
        "\n",
        " \n",
        "\n",
        "3. __host__: indica que esta función pertenece al host es decir que esta función solo puede ser ejecutada en el host. En otras palabras es una función de C tradicional.\n",
        "\n",
        "Algunas de las variables reservadas de Cuda y que son de gran utilidad son las siguientes.\n",
        "\n",
        " \n",
        "\n",
        "gridDim: indica las dimensiones del Grid.\n",
        "\n",
        " \n",
        "\n",
        "blockIdx: indica el identificador del Bloque de un Grid.\n",
        "\n",
        " \n",
        "\n",
        "blockDim: contiene las dimensiones de un Bloque.\n",
        "\n",
        " \n",
        "\n",
        "threadIdx: contiene el identificador del Thread dentro del Bloque.\n",
        "\n",
        " \n",
        "\n",
        "Es importante notar que todas estas variables contienen componentes en X, Y y Z. Los Grids y los Bloques pueden ser de 1, 2 o 3 dimensiones.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LAe7md8O68I"
      },
      "source": [
        "#Compilación y Ejecución.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5UDZ9GxPAhX"
      },
      "source": [
        "##Compilar y Ejecutar.\n",
        "\n",
        "Una vez instalado el API de Cuda para el respectivo hardware la forma de compilar y ejecutar código de Cuda es muy similar al lenguaje C.\n",
        "\n",
        " \n",
        "\n",
        "Supongamos que ya se cuenta con el codigo fuente de hola mundo para cuda (hello.cu)\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "*   Para compilar: \\$nvcc hello.cu -o (hello.o) codigo\n",
        "\n",
        "\n",
        "El comando anterior compila (y en caso de no haber errores) y genera un archivo ejecutable (binario) que puede ser ejecutado de la siguiente manera:\n",
        "\n",
        " \n",
        "*   Para ejecutar:\\$./codigo\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfteTdToPWyV"
      },
      "source": [
        "#Glosario.\n",
        "\n",
        "Dispositivo: En el contexto de Cuda un dispositivo hace referencia a un GPU.\n",
        "\n",
        " \n",
        "\n",
        "Asíncrono: En computación un evento (proceso) asíncrono es aquel no tiene correspondencia temporal con otro evento. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHU_RuNSPbRQ"
      },
      "source": [
        "#Referencias.\n",
        "\n",
        "1. Tolga Soyata: GPU Parallel Program Development Using Cuda.\n",
        "\n",
        "2. https://fisica.cab.cnea.gov.ar/gpgpu/images/clases/clase_1_cuda.pdf\n",
        "\n",
        "3. Dongarra Foster: Source Book of parallel computing."
      ]
    }
  ]
}