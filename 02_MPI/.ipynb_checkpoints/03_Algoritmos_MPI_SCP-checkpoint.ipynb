{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jugernaut/MACTI-programacionparalelo/blob/main/02_MPI/03_Algoritmos_MPI_SCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gde4FA2b-KeX"
   },
   "source": [
    "<font color=\"Teal\" face=\"Comic Sans MS,arial\">\n",
    "  <h1 align=\"center\"><i>Algoritmos MPI</i></h1>\n",
    "  </font>\n",
    "  <font color=\"Black\" face=\"Comic Sans MS,arial\">\n",
    "  <h5 align=\"center\"><i>Profesor: M. en C. Miguel Angel Pérez León</i></h5>\n",
    "  <h5 align=\"center\"><i>Ayudante: Lucía Martínez Rivas</i></h5>\n",
    "  <h5 align=\"center\"><i>Ayudante: Erick Jesús Rios Gonzalez</i></h5>\n",
    "  <h5 align=\"center\"><i>Materia: Seminario de programación en paralelo</i></h5>\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqJZuAbB-KeZ"
   },
   "source": [
    "# Introducción\n",
    "\n",
    "Ya que hemos visto algunos métodos interesantes previamente (Monte Carlo), el siguiente paso consiste en convertir la versión secuencial de estos algoritmos a su versión en paralelo.\n",
    "\n",
    "Para ello vamos a comenzar con el ejemplo básico de la aproximación de $\\pi$ empleando métodos de Monte Carlo, tal como se mostró en la presentación previa, en la cual se describe el desarrollo y marco teórico de dicho método.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHFG33_0-Keb"
   },
   "source": [
    "# Aproximación de $\\pi$ mediante Monte Carlo\n",
    "\n",
    "Para la implementación de este algoritmo en paralelo utilizaremos *MPI*, ya que tanto el método como el algoritmo son ideales para tal propósito.\n",
    "\n",
    "Vale la pena aclarar que la idea de los métodos de Monte Carlo se basan en la generación de números aleatorios y es por esto que *MPI* es muy buena idea para este tipo de algoritmos, ya que *MPI* originalmente se creo para arquitecturas sin memoria compartida (envío de mensajes).\n",
    "\n",
    "A continuación podemos ver el algoritmo para la aproximación de $\\pi$ haciendo uso de *MPI*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "BoVbQ3wTrpig"
   },
   "outputs": [],
   "source": [
    "codigo = \"\"\"\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <mpi.h>\n",
    "#include <math.h>\n",
    "#include <time.h>\n",
    "void Validar_Entrada(int argc, int id);\n",
    "long Lanzamientos(long numLanzamientoProc, int miRango);\n",
    " \n",
    "int main(int argc, char* argv[]){\n",
    "    // Bloque de variables\n",
    "    int lan, id_proceso, num_procesos, total_dardos_dentro, dardo_dentro_proc, lanzamiento_proceso;\n",
    "    double tiempo_local, tiempo_final, tiempo_inicial, transcurrido, N, pi_aprox;\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &id_proceso);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &num_procesos);\n",
    "    // se valida la entrada del usuario\n",
    "    Validar_Entrada(argc, id_proceso);\n",
    "    // Se guarda el numero total de lanzamientos\n",
    "    sscanf(argv[1], \"%lf\", &N);\n",
    "    // Bloqueo para esperar que cada proceso realice esta parte\n",
    "    MPI_Barrier(MPI_COMM_WORLD);\n",
    "    // tiempo inicial\n",
    "    tiempo_inicial = MPI_Wtime();\n",
    "    // lanzamiento por proceso\n",
    "    lanzamiento_proceso = N/num_procesos;\n",
    "    // CADA PROCESO SE ENCARGA DE REALIZAR ESTA SECCION EN PARALELO\n",
    "    dardo_dentro_proc = Lanzamientos(lanzamiento_proceso, id_proceso);\n",
    "    // tiempo final\n",
    "    tiempo_final = MPI_Wtime();\n",
    "    // tiempo que tomo en cada procesador\n",
    "    tiempo_local = tiempo_final-tiempo_inicial;\n",
    "    // SUMA de cada dardo dentro de la circunferencia\n",
    "    MPI_Allreduce(&dardo_dentro_proc, &total_dardos_dentro, 1, MPI_INT, MPI_SUM, \n",
    "    MPI_COMM_WORLD);\n",
    "    // NOS QUEDAMOS CON EL MAXIMO DE LOS TIEMPOS, ya que eso fue lo que tomo\n",
    "    MPI_Allreduce(&tiempo_local, &transcurrido, 1, MPI_DOUBLE, MPI_MAX, \n",
    "    MPI_COMM_WORLD); \n",
    "    // el proceso maestro se encarga de imprimir el resultado\n",
    "    if (id_proceso == 0) { \n",
    "      pi_aprox = (total_dardos_dentro*4)/((double)N);\n",
    "      lan = (int)N;\n",
    "      printf(\"Numero de lanzamientos:      \t    %d\\\\n\", lan);\n",
    "      printf(\"Valor aproximado de pi:       %24.16f\\\\n\", pi_aprox);\n",
    "      printf(\"Maximo tiempo transcurrido en el lanzamiento de dardos:  %.16f\\\\n\", \n",
    "      transcurrido*1000000);\n",
    "    }\n",
    "    // se liberan los recursos utilizados\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "// Funciona que valida la entrada del usuario\n",
    "void Validar_Entrada(int argc, int id){\n",
    "   // Validacion de la entrada\n",
    "   if(argc !=2){\n",
    "      if (id==0){\n",
    "        fprintf(stderr,\"Numero incorrecto de parmetros\\\\n\");\n",
    "        fflush(stderr);\n",
    "      }\n",
    "   // se termina la ejecucion del programa\n",
    "   MPI_Abort(MPI_COMM_WORLD,1);\n",
    "   }  \n",
    "}\n",
    "\n",
    "// Funcion que simula los lanzamientos de los dardos\n",
    "long Lanzamientos(long numLanzamientoProc, int id_proc){\n",
    "    // variables para el conteo de lazamientos de dardos\n",
    "    long lanzamiento, numeroEnCirculo = 0;        \n",
    "    double x,y;\n",
    "    // semilla para los valores aleatorios\n",
    "    unsigned int semilla = (unsigned) time(NULL);\n",
    "    srand(semilla + id_proc);\n",
    "    // cada proceso lanza genera sus respectivos valores aleatorios\n",
    "    for (lanzamiento = 0; lanzamiento < numLanzamientoProc; lanzamiento++) {\n",
    "      x = rand_r(&semilla)/(double)RAND_MAX;\n",
    "      y = rand_r(&semilla)/(double)RAND_MAX;\n",
    "    // si el lanzamiento cayo en la circunferencia se incrementa el contador \n",
    "    if((x*x+y*y) <= 1.0 ) numeroEnCirculo++;\n",
    "    }\n",
    "    return numeroEnCirculo;  \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# se crea el archivo con permisos para escribir mediante python\n",
    "archivo_texto = open(\"aprox_PI.c\", \"w\")\n",
    "# se escribe el programa en el archivo \n",
    "archivo_texto.write(codigo)\n",
    "# se cierra el buffer de escritura\n",
    "archivo_texto.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "-9VZmsm3sANU"
   },
   "outputs": [],
   "source": [
    "!mpicc aprox_PI.c -o aprox_PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16RmriMOsSIw",
    "outputId": "6677e5d6-f20f-4f5e-b7d8-977172204686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de lanzamientos:      \t    10000\n",
      "Valor aproximado de pi:             3.1600000000000001\n",
      "Maximo tiempo transcurrido en el lanzamiento de dardos:  11.1829999696055893\n"
     ]
    }
   ],
   "source": [
    "!mpirun --allow-run-as-root -np 100 ./aprox_PI 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYA0Fal3Z3Jt"
   },
   "source": [
    "## Análisis a secciones importantes\n",
    "\n",
    "*MPI_Abort(MPI_COMM_WORLD,1)*: en caso de que el usuario no introduzca los valores correctos el programa finaliza y todos los procesos asociados al mismo.\n",
    "\n",
    "*MPI_Barrier(MPI_COMM_WORLD)*: esta directiva obliga a que todos los procesos asignados a este procesador terminen antes de poder avanzar.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://github.com/jugernaut/ProgramacionEnParalelo/blob/main/Imagenes/MPI/barrier.png?raw=1\" width=\"700\">\n",
    "</center>\n",
    "\n",
    "*MPI_Allreduce(&dardo_dentro_proc, &total_dardos_dentro, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD)*: funciona de manera similar al *reduce* de *OpenMP* en el cual se realiza la reduccion de multiples variables en una sola.\n",
    "\n",
    "*MPI_Allreduce(&tiempo_local, &transcurrido, 1, MPI_DOUBLE, MPI_MAX, \n",
    "MPI_COMM_WORLD)*: a diferencia de la función anterior, en este caso nos quedamos con el máximo de los tiempos que le tomó a cada proceso realizar la tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0zl8srO-Kef"
   },
   "source": [
    "# Suma de los primeros $n$ números naturales\n",
    "\n",
    "En esta sección vamos a analizar un algoritmo ampliamente conocido y hasta cierto punto sencillo, sin embargo muy didáctico y adecuado para mostrar las cualidades de la programación en paralelo. La suma de los primeros $n$ números naturales es un ejemplo ideal para mostrar el concepto de **pase de mensajes** entre procesos.\n",
    "\n",
    "A primera vista esta labor puede parecer algo trivial.\n",
    "\n",
    "$$\\sum_{i=1}^{n}i=\\frac{n(n+1)}{2}$$\n",
    "\n",
    "Sin embargo la complejidad de esta sección radica en, ¿cómo dividir esta labor en diferentes secciones en paralelo para poder reducir el tiempo de ejecución. La idea sería difundir entre todos los procesos (*broadcast*) el vector de elementos y realizar la suma parcial en cada proceso respectivo.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://github.com/jugernaut/ProgramacionEnParalelo/blob/main/Imagenes/MPI/broadcast.png?raw=1\" width=\"700\">\n",
    "</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "cJmwGXOEhztj"
   },
   "outputs": [],
   "source": [
    "codigo = \"\"\"\n",
    "#include <stdio.h>\n",
    "#include <mpi.h>\n",
    "// cantidad de valore a sumar\n",
    "#define tam 10000\n",
    "\n",
    "int main (int argc, char** argv){\n",
    "   // bloque de variables globales\n",
    "   int id_proceso, num_procesos, a[tam], i, primero, ultimo, tam_porcion;\n",
    "   double suma_total = 0, suma_parcial = 0;\n",
    "   // funciones para iniciar uso de MPI\n",
    "   MPI_Status status;\n",
    "   MPI_Init(&argc, &argv);\n",
    "   MPI_Comm_rank(MPI_COMM_WORLD, &id_proceso);\n",
    "   MPI_Comm_size(MPI_COMM_WORLD, &num_procesos);\n",
    "   // el proceso root (maestro) llena el vector a sumar\n",
    "   if (id_proceso == 0) \n",
    "      for (i=0;i<tam;i++)\n",
    "         a[i] = i + 1;\n",
    "   // se transmite (BROADCAST) el vector a todos los procesos\n",
    "   MPI_Bcast(a,tam,MPI_INT,0,MPI_COMM_WORLD);\n",
    "   // determinamos el tamano de la seccion a sumar\n",
    "   tam_porcion = tam/num_procesos;\n",
    "   // en caso de ser el ultimo procesador se asignan los limites\n",
    "   if (id_proceso == num_procesos - 1) {\n",
    "      primero = (num_procesos - 1)*tam_porcion;\n",
    "      ultimo = tam - 1;\n",
    "   } // se define el indice inicial y final dependiendo del id_proceso\n",
    "   else {\n",
    "      primero = id_proceso*tam_porcion;\n",
    "      ultimo = (id_proceso + 1)*tam_porcion - 1;\n",
    "   }\n",
    "   // CADA PROCESO se realiza su respectiva suma parcial\n",
    "   for (i=primero; i<=ultimo; i++) \n",
    "      suma_parcial = suma_parcial + a[i];\n",
    "   // se juntan todas las sumas parciales\n",
    "   MPI_Reduce(&suma_parcial, &suma_total, 1, MPI_DOUBLE, MPI_SUM, 0, \n",
    "   MPI_COMM_WORLD);\n",
    "   if (id_proceso == 0) \n",
    "      printf(\"La suma es: %10.0f\\\\n\", suma_total);\n",
    "   // se liberan los recursos utilizados\n",
    "   MPI_Finalize();\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "# se crea el archivo con permisos para escribir mediante python\n",
    "archivo_texto = open(\"sum_n.c\", \"w\")\n",
    "# se escribe el programa en el archivo \n",
    "archivo_texto.write(codigo)\n",
    "# se cierra el buffer de escritura\n",
    "archivo_texto.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "xLKd0YH__eRn"
   },
   "outputs": [],
   "source": [
    "!mpicc sum_n.c -o sum_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wwDpoceW_nOT",
    "outputId": "6df08ab6-811f-4a60-b5c9-7318d0508693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La suma es:   50005000\n"
     ]
    }
   ],
   "source": [
    "!mpirun --allow-run-as-root -np 100 ./sum_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJG5lGDp_2HN"
   },
   "source": [
    "## Nuevas funciones (*broadcast*)\n",
    "\n",
    "Una de las nuevas funciones es la función *MPI_Bcast(a,tam,MPI_INT,0,MPI_COMM_WORLD);*, está se encarga de difundir (broadcast) el mensaje que incluyen los datos a procesar.\n",
    "\n",
    "Una vez que cada proceso tiene la información pertinente, solo es cuestión de procesarla (realizar la suma parcial) y finalmente devolver el resultado mediante *reduce*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mi2famUv-Kex"
   },
   "source": [
    "# Referencias\n",
    "\n",
    "*   [Teorema Fundamental de Monte Carlo](https://www.ugr.es/~jillana/Docencia/FM/mc.pdf).\n",
    "*   [Ejemplos MPI (mallas)](https://htor.inf.ethz.ch/teaching/mpi_tutorials/ppopp13/2013-02-24-ppopp-mpi-basic.pdf)\n",
    "*   [Ejemplos MPI (juego de la vida)](https://cimec.org.ar/~mstorti/curso-mpi-petsc/slides.pdf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "03_Algoritmos_MPI_SCP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
