{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Envoltorios_SCP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugernaut/ProgramacionEnParalelo/blob/desarrollo/Envoltorios/Envoltorios_SCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCy4sQbk6TDS"
      },
      "source": [
        "<font color=\"Teal\" face=\"Comic Sans MS,arial\">\n",
        "  <h1 align=\"center\"><i>Envoltorios (Wrappers)</i></h1>\n",
        "  </font>\n",
        "  <font color=\"Black\" face=\"Comic Sans MS,arial\">\n",
        "  <h5 align=\"center\"><i>Profesor: M.en.C. Miguel Angel Pérez León</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Jesús Iván Coss Calderón.</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Mario Arturo Nieto Butron</i></h5>\n",
        "  <h5 align=\"center\"><i>Materia: Seminario de programación en paralelo</i></h5>\n",
        "  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsaIeAtF6T1G"
      },
      "source": [
        "# Introducción\n",
        "\n",
        "Una vez que ya se conocen las principales *API's* para programar en paralelo como *OpenMP*, *MPI* o *CUDA*, así como sus ventajas y desventajas podemos comenzar a utilizar alternativas como lo son los ***wrappers*** (envoltorios).\n",
        "\n",
        "Un *wrapper* es un conjunto de librerías y herramientas (en otro lenguaje diferente a C) que actúa como puente y oculta muchos de los detalles de este tipo de *API's*.\n",
        "\n",
        "Existe una infinidad de lenguajes de alto nivel que permiten hacer uso de estos *wrappers*, como lo son *JAVA*, *Python*, *R*, etc.\n",
        "\n",
        "Para esta presentación nos enfocaremos en el lenguaje *Python* y algunos de los *wrappers* que existen en este lenguaje ya que las ventajas que ofrece este lenguaje lo hacen ideal para su uso en este curso.\n",
        "\n",
        "Dos de los envoltorios más populares para *Python* son *Numba* y *TensorFlow*.\n",
        "\n",
        "A pesar de la gran cantidad de *wrappers* que existen actualmente, debido a los alcances del curso, solo podremos revisar *Numba* y *TensorFlow*.\n",
        "\n",
        "Aquí podemos ver las diferentes capas que se construyen con *python*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/wrapper.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Los 2 diferentes enfoques que se les puede dar a los envoltorios.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/arribabajo.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Flujo de numba, ¡como es que se optimiza el código!.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/numba.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/numba-arch.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Relación entre *tensorflow* y *Nvidia*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/tensor2.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/tensor1.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Capas en el desarrollo de *software*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/tensordevice.jpg?raw=1\" width=\"600\"> \n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PPn3oSp8WaM"
      },
      "source": [
        "# *Numba*\n",
        "\n",
        "*Numba* es uno de los envoltorios más conocidos y empleados actualmete, debido a que su funcionamiento es muy sencillo así como su instalación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOiTvC5u8cQP"
      },
      "source": [
        "## ¿Cómo funciona?\n",
        "\n",
        "*Numba* tiene multiples formás de optimizar codigo y lograr que este funcione de manera más rápida, esto lo realiza mediante alguna de las siguientes variantes:\n",
        "\n",
        "• Convierte código *Python* en código de máquina: al compilar código empleando numba, este convierte el código en código de maquina y la segunda vez que sea ejecutado este mostrara un mejor desempeño que se traduce en una ejecución más rápida.\n",
        "\n",
        "• Es posible utilizar una capa (*layer*) para acceder a características de *OpenMP*.\n",
        "\n",
        "• Es posible paralelizar código empleando utilidades de *MPI*.\n",
        "\n",
        "• Tiene soporte para el uso de *GPU's* utilizando *CUDA* como *background*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PehjbuHF8sKR"
      },
      "source": [
        "## Ventajas\n",
        "\n",
        "*Numba* posee múltiples ventajas, aunque una de las más importantes es poder decidir como optimizar el código escrito en python.\n",
        "\n",
        "Es muy sencillo de instalar mediante *pip* e igual de fácil de usar que *Python*.\n",
        "\n",
        "Se tiene una gran capacidad de acoplamiento con *numpy* (biblioteca para computo cientifico).\n",
        "\n",
        "Además de ser posible optar por un mecanismo para optimizar el código, *numba* permite escribir código híbrido que combine lo mejor de las diferentes formás de optimizar el desempeño.\n",
        "\n",
        "Emplear *numba* es tan sencillo como importar la biblioteca y hacer uso de sus **decoradores** para optimizar el código.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJhW1gUa9DdF"
      },
      "source": [
        "## Desventajas\n",
        "\n",
        "*Numba* tiene en realidad muy pocas desventajas.\n",
        "\n",
        "La más evidente de estas, es que **encapsula mucho de su funcionamiento**, es decir que en realidad funciona como caja negra.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asKyxjQD9Pip"
      },
      "source": [
        "## Instalación en equipo local\n",
        "\n",
        "Dado que a partir de este punto del curso es posible emplear diferentes versiones de *Python* o de sus bibliotecas, se recomienda crear un **entorno virtual** y en este entorno instalar *numba*.\n",
        "\n",
        "Supongamos que ya se cuenta *virtualenv*, *Python* 2.7 y *pip*.\n",
        "\n",
        "1.   Crear entorno virtual: \n",
        "\n",
        "*\\$mkdir numba*\n",
        "\n",
        "*\\$virtualenv numba*\n",
        "\n",
        "2.   Activar entorno virtual:\n",
        "\n",
        "*\\$source numba/bin/activate*\n",
        "\n",
        "3.   Instalar *numba*:\n",
        "\n",
        "*(numba)\\$pip install numba*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg_-5MXa94pG"
      },
      "source": [
        "### Decoradores\n",
        "\n",
        "Un decorador en *numba* es una forma de modificar funciones de manera tal que pueda ser optimizada empleando alguna de las técnicas previamente mencionadas.\n",
        "\n",
        "Se puede pensar en un decorador en una función que recibe una función como parámetro y devuelve otra función optimizada como salida.\n",
        "\n",
        "Una función de *python* es envuelta por uno o más decoradores, una vez que se define esta función el decorador es evaluado y *numba* devuelve una función optimizada que puede ser invocada desde *python*.\n",
        "\n",
        "El alcance del o de los decoradores se limita al alcance de la función definida a la cual se le aplique dichos decoradores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY9qCM04-zEJ"
      },
      "source": [
        "### *nopython*\n",
        "\n",
        "La forma en la se utiliza un decorador es con la sentencia *@jit*(parametros).\n",
        "\n",
        "La forma más básica en la cual se puede usar *numba*, es mediante el decorador *nopython=True*.\n",
        "\n",
        "Esta sentencia lo que le indica a *numba* es que el código en el cual esta envuelta la función, debe se compilado y ejecutado sin utilizar el entono de *python*. Lo que significa que una vez que ha sido compilada esta función se ejecutara de manera más eficiente que empleando el interprete de *python*. \n",
        "\n",
        "Existe otro modo de compilación conocido como object mode, y se accede a este cuando no se hace uso de *nopython=True*. Sin embargo este modo se limita a optimizar unicamente los ciclos y no todo el código definido en la función.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0LL39Jv-9l8"
      },
      "source": [
        "### *parallel=True*\n",
        "\n",
        "Otro decorador muy útil pero a la vez 'obscuro' es, *@njit(parrallel=True)*.\n",
        "\n",
        "Este decorador va de la mano de la palabra reservada prange y en conjunto permiten ejecutar en paralelo ciclos dentro de la función definida.\n",
        "\n",
        "Este decorador oculta mucho del proceso que se realiza al ejecutar un algoritmo en paralelo. Sin embargo ya que a esta altura del curso se conoce cual es el transfondo (*OpenMP, MPI, CUDA*), podemos obviar el mismo.\n",
        "\n",
        "La parlabra reservada prange se emplea para especificar el ciclo que se quiere realizar en paralelo y no solo eso, también realiza la operación conocida como reduction de alguna variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uifxfic__Wcw"
      },
      "source": [
        "## *Numba* + *CUDA*\n",
        "\n",
        "*Numba* ofrece soporte para programación de *GPU* mediante *CUDA*, permitiendo compilar un subconjunto restringido de código escrito en python que se traduce en funciones tipo *kernel* y tipo device.\n",
        "\n",
        "Una característica importante de *numba*, es que al definir funciones de tipo *kernel*, *numba* hace parecer que esa función tiene acceso directo a arreglos de tipo *numpy*. Los arreglos de tipo *numpy* que se pasan como parámetro a las funciones de tipo kernel se transfieren de forma automática entre la memoria del *CPU* y del *GPU*.\n",
        "\n",
        "*Numba* no tiene una implementación directa para todo el *API* de *CUDA*, de tal forma que algunas características de *CUDA* no son accesibles desde *numba*. Sin embargo las funciones definidas en *numba* son suficientes para comenzar a desarrollar algoritmos que hagan uso del o de los *GPU's* de una computadora.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hiK7syt_wgx"
      },
      "source": [
        "### Declaración y uso de un kernel\n",
        "\n",
        "La misma terminología empleada en el desarrollo de código usando *CUDA*, se aplica para el desarrollo de código mediante *numba*.\n",
        "\n",
        "• Una función de tipo *kernel* no puede devolver un tipo de manera explicita; cualquier resultado de la función *kernel* debe ser almacenado en el arreglo de tipo *numpy* que se pasa coma parámetro a esta función.\n",
        "\n",
        "• Cuando se ejecuta un *kernel* se debe declarar de manera explicita la jerarquía de hilos, es decir; el número de bloques de hilos y el número de hilos por bloque.\n",
        "\n",
        "• Es importante notar que un *kernel* se compila una sola vez, pero puede ser llamado con diferentes tamaños de bloque o de *grid*.\n",
        "\n",
        "• En caso de tener acceso a una tarjeta Nvidia, es posible emplear el simulador de [CUDA](https://nyu-cds.github.io/python-numba/05-cuda/) .\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3WQ33GHAwOz"
      },
      "source": [
        "### Observaciones y Recomendaciones\n",
        "\n",
        "Por default la ejecución del *kernel* se realiza de manera síncrona; la función termina cuando el *kernel* ha terminado su ejecución y los datos son persistentes.\n",
        " \n",
        "Para elegir el tamaño del bloque, es decir el numero de hilos por bloque hay que considerar 2 cosas: \n",
        "\n",
        "1.   Del lado del software: el tamaño del bloque determina cuantos hilos comparten memoria.\n",
        "2.   Del lado del hardware: el tamaño del bloque debe ser suficientemente grande para ocupar todas las unidades de ejecución.\n",
        "\n",
        "Sugerencias para identificar el tamaño del bloque pueden encontrarse en este sitio [https://docs.nvidia.com/cuda/cuda-c-programming-guide/]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geIA0krQ2-VZ"
      },
      "source": [
        "## *Numba* en *google colab*\n",
        "\n",
        "Para utilizar numba en google colab, es tan sencillo como importar la biblioteca y utilizar sus decoradores, a continuación se muestra el ejemplo de la aproximación de $pi$ optimizado mediante *numba*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brKZ9XmT3VnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efcfcf7f-d9d4-4b30-d0ed-260cd4f1deb1"
      },
      "source": [
        "import random\n",
        "from timeit import default_timer as timer\n",
        "from numba import jit, njit\n",
        "\n",
        "#descomentar para optimizar\n",
        "#@jit(nopython=True)\n",
        "def mc_pi_aprox(n=100000000):\n",
        "    dentro_circulo = 0 \n",
        "    for i in range(n):\n",
        "      x = random.random()\n",
        "      y = random.random()\n",
        "      # valores dentro de la circunferencia\n",
        "      if (x**2+y**2 < 1):\n",
        "          dentro_circulo += 1\n",
        "    return 4*dentro_circulo / n\n",
        "\n",
        "# inicial\n",
        "inicio = timer()\n",
        "# algoritmo\n",
        "mc_pi_aprox()\n",
        "# final\n",
        "final = timer()\n",
        "\n",
        "# tiempo\n",
        "print('Tomo:',final - inicio, 'segundos')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tomo: 44.45181329099998 segundos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stEwY6wIBNBZ"
      },
      "source": [
        "# *Tensorflow*\n",
        "\n",
        "*TensorFlow* es un conjunto de herramientas que proporciona *Google* para el desarrollo de algoritmos de aprendizaje automático, algunas de sus características son:\n",
        "\n",
        "• Cuenta con diferentes versiones de su *API* para lenguajes tales como: *C++, Haskell, Java* y *Go* entre algunos, aunque la versión más usada es la de *python*.\n",
        "\n",
        "• Existen versiones optimizadas de *tensorflow* que hacen uso de programación mediante *GPU's* o incluso mediante *TPU's*.\n",
        "\n",
        "• Una de sus aplicaciones más comunes es en el desarrollo de redes neuronales.\n",
        "\n",
        "• El desarrollo de *tensorflow* es mediante licencia de código abierto, lo que significa que no hace falta pagar una licencia para hacer uso del mismo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlEvJkJPBUtk"
      },
      "source": [
        "## Aplicaciones\n",
        "\n",
        "*TensorFlow* tiene bastas aplicaciones, algunas de ellas son:\n",
        "\n",
        "• I.A. detrás de las fotografiás en *smarthphones*: recientemente se ha empleado técnicas de i.a. para mejorar la captura de imágenes en un *smartphone*, detrás de esta i.a podemos encontrar bibliotecas como *tensorflow*.\n",
        "\n",
        "• Diagnostico médico: *tensorflow* ya está mejorando las herramientas que utilizan los médicos, por ejemplo ayudando a analizar radiografías o fotografiás de pacientes y sugiriendo un diagnostico casi de inmediato.\n",
        "\n",
        "• Procesamiento de imágenes: una de las aplicaciones más conocidas de *tensorflow* es el *software* automatizado de procesamiento de imágenes, ***DeepDream*** es de los ejemplos mas conocidos al respecto. \n",
        "\n",
        "• El desarrollo de *tensorflow* es mediante licencia de código abierto, lo que significa que no hace falta pagar una licencia para hacer uso del mismo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX6YRj3mBavU"
      },
      "source": [
        "## ¿Cómo funciona?\n",
        "\n",
        "La idea de *tensorflow* es definir el cómputo de datos como una gráfica conformada por nodos y tensores (aristas).\n",
        "\n",
        "• Grafo: un grafo es un conjunto de nodos y aristas que representan el cómputo de información que se recibe como entrada.\n",
        "\n",
        "• Nodo: en el contexto de *tensorflow*, un nodo es conocido como una operación (op), la cual recibe uno o mas tensores y realiza la operación indicada.\n",
        "\n",
        "• Tensor (arista): es una agrupación de datos que puede tomar diferentes formas dependiendo del rango; rango 0 es un escalar, rango 1 es un vector, rango 2 una matriz, etc.\n",
        "\n",
        "• Sesión: para poder realizar los cálculos definidos en el grafo, se debe llevar a cabo mediante una sesión que representa el cálculo que se desea realizar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgF-TNuVBjec"
      },
      "source": [
        "##Visualización\n",
        "\n",
        "Podemos pensar en un grafo de la siguiente forma.\n",
        "\n",
        "Un ejemplo de grafo es el siguiente.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/op1.png?raw=1\" width=\"450\"> \n",
        "</center>\n",
        "\n",
        "Un perceptron simple lo podemos imaginar de la siguiente forma.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/op2.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Una capa de una red neuronal se ve de esta manera.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/opn.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "Finalmente una red neuronal (SOM), la podemos pensar así.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Wrappers/som.gif?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6qgBqlwCGJO"
      },
      "source": [
        "##  Instalación en equipo local\n",
        "\n",
        "Supongamos que ya se cuenta *virtualenv, python 2.7 y pip*.\n",
        "\n",
        "1.   Crear entorno virtual: \n",
        "\n",
        "\\$mkdir tensorflow\n",
        "\n",
        "\\$virtualenv tensorflow *texto en cursiva*\n",
        "\n",
        "2.   Activar entorno virtual:\n",
        "\n",
        "*\\$source tensorflow/bin/activate*\n",
        "\n",
        "3.   Instalar *tensorflow*:\n",
        "\n",
        "*(numba)\\$pip install tensorflow==2.0*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBIJYwmg48Zr"
      },
      "source": [
        "## *Tensorflow* en *google colab*\n",
        "\n",
        "En caso de usar la versión mas reciente de tensorflow, sucede igual que con numba, solo es necesario importar la biblioteca y hacer uso de la misma, en otro caso es necesario desintalar la versión actual e instalar la versión necesaria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9fB90WK51MJ"
      },
      "source": [
        "# Otros envoltorios\n",
        "\n",
        "Existe una gran variedad de envoltorios para multiples lenguajes, por ejemplo la clase *Thread* o la interfaz *Runnable* del lenguaje Java.\n",
        "\n",
        "Un ejemplo de lo que se puede realizar con este tipo de envoltorios es el entorno de [*NetLogo*](http://www.netlogoweb.org/launch#http://www.netlogoweb.org/assets/modelslib/Sample%20Models/Art/Follower.nlogo), herramienta/lenguaje de simulación de modelos basados en agentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dsaFmbyCamn"
      },
      "source": [
        "# Glosario\n",
        "\n",
        "*Layer*: Capa informática, nivel o capa que se oculta una parte del *software*.\n",
        "\n",
        "*Background*: En computación entorno que da soporte a un determinado software. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J_aD2oaChF-"
      },
      "source": [
        "# Referencias\n",
        "\n",
        "1. https://nyu-cds.github.io/python-numba/01-jit/\n",
        "\n",
        "2. https://christophdeil.com/download/2019-07-11_Christoph_Deil_Numba.pdf\n",
        "\n",
        "3. https://numba.pydata.org/numba-doc/dev/cuda/kernels.html\n",
        "\n",
        "4. Tolga Soyata:\\newblock GPU Parallel Program Development Using CUDA."
      ]
    }
  ]
}
