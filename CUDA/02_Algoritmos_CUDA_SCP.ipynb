{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Algoritmos_CUDA_SCP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugernaut/ProgramacionEnParalelo/blob/desarrollo/CUDA/02_Algoritmos_CUDA_SCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rYgUDA8Ltm4"
      },
      "source": [
        "<font color=\"Teal\" face=\"Comic Sans MS,arial\">\n",
        "  <h1 align=\"center\"><i>Algoritmos CUDA</i></h1>\n",
        "  </font>\n",
        "  <font color=\"Black\" face=\"Comic Sans MS,arial\">\n",
        "  <h5 align=\"center\"><i>Profesor: M.en.C. Miguel Angel Pérez León</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Jesús Iván Coss Calderón</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Mario Arturo Nieto Butron</i></h5>\n",
        "  <h5 align=\"center\"><i>Materia: Seminario de programación en paralelo</i></h5>\n",
        "  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTIY01-ELvbH"
      },
      "source": [
        "# Introducción\n",
        "\n",
        "Como ya se menciono previamente, *CUDA* hace uso de las *GPU's* (dispositivos de cómputo de propósito especifico), estos dispositivos están optimizados para trabajar con imágenes y resulta que una imagen dentro de una computadora se representa mediante elementos matemáticos como matrices o vectores.\n",
        "\n",
        "De hecho la mayoría de los formatos de imágenes más comunes (*.jpg, .jpeg, .png*) consideran a la imagen como una matriz de pixeles o un mapa de bits.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/ProgramacionEnParalelo/blob/desarrollo/Imagenes/CUDA/smile.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyAG-V_3MwrN"
      },
      "source": [
        "# Suma de Vectores\n",
        "\n",
        "El algoritmo más sencillo que podemos comenzar a analizar es la suma de vectores y aunque suene trivial, la realidad es que hasta antes de este momento toda suma de vectores que hayas realizado previamente **se ejecuto de manera secuencial**, lo cuál significa un desperdicio de recursos.\n",
        "\n",
        "Gracias a *CUDA* (aunque también se puede realizar con *OpenMP* y *MPI*) esta operación elemental se puede realizar en paralelo y gracias a esto optimizar recursos, lo que se traduce en un menor tiempo de ejecución."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6YKyaVxM19I"
      },
      "source": [
        "## ¿Cómo funciona?\n",
        "\n",
        "La forma tradicional de como funciona la suma de vectores, se basa en la definición formal \"se realiza entrada a entrada\", sin embargo nunca nos dijeron que está \"suma entrada a entrada\" se puede realizar en paralelo.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/ProgramacionEnParalelo/blob/desarrollo/Imagenes/CUDA/sumavect.png?raw=1\" width=\"600\"> \n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSg1edoNAwsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7329a12c-2385-4f06-e3b8-dca899e2cc06"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-65f__9o_\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-65f__9o_\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4306 sha256=b6a3c57faef1741bbfc0beb65075e70de5172fb776f47257c697bb4ad63ea4a4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-27gq_8jy/wheels/c5/2b/c0/87008e795a14bbcdfc7c846a00d06981916331eb980b6c8bdf\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HGthAMmxdLf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4179b2ef-1448-406b-971e-a4468044b204"
      },
      "source": [
        "%%cu\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <stdio.h>\n",
        "\n",
        "// Kernel (funcion) que se invoca desde el Host y se ejecuta en un dispositivo\n",
        "__global__ void suma_vectores(int* c, const int* a, const int* b, int size) {\n",
        "    // vector de direccionamiento\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < size) {\n",
        "        c[i] = a[i] + b[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Funcion auxiliar que encapsula la suma con CUDA\n",
        "void suma_CUDA(int* c, const int* a, const int* b, int tam) {\n",
        "    int* dev_a = nullptr;\n",
        "    int* dev_b = nullptr;\n",
        "    int* dev_c = nullptr;\n",
        "\n",
        "    // Reservamos espacio de memoria para los datos, 2 de entrada y una salida\n",
        "    cudaMalloc((void**)&dev_c, tam * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_a, tam * sizeof(int));\n",
        "    cudaMalloc((void**)&dev_b, tam * sizeof(int));\n",
        "\n",
        "    // Copiamos los datos de entrada desde el CPU a la memoria del GPU\n",
        "    cudaMemcpy(dev_a, a, tam * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, tam * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Se invoca al kernel en el GPU con un hilo por cada elemento\n",
        "    // 2 es el numero de bloques y (tam + 1)/2 es el numero de hilos en cada bloque\n",
        "    suma_vectores<<<2, (tam + 1) / 2>>>(dev_c, dev_a, dev_b, tam);\n",
        "    \n",
        "    // Esta funcion espera a que termine de ejecutarse el kernel y \n",
        "    // devuelve los errores que se hayan generado al ser invocado\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copiamos el vector resultado de la memoria del GPU al CPU\n",
        "    cudaMemcpy(c, dev_c, tam * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Se libera la memoria empleada\n",
        "    cudaFree(dev_c);\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "}\n",
        "\n",
        "// Funcion principal que sirve de prueba para el algoritmo\n",
        "int main(int argc, char** argv) {\n",
        "    \n",
        "    // Datos de entrada para nuestra funcion\n",
        "    const int tam = 5;\n",
        "    const int a[tam] = {  1,  2,  3,  4,  5 };\n",
        "    const int b[tam] = { 10, 20, 30, 40, 50 };\n",
        "    int c[tam] = { 0 };\n",
        "\n",
        "    // Se llama a la funcion que encapsula el Kernel\n",
        "    suma_CUDA(c, a, b, tam);\n",
        "\n",
        "    // Mostramos resultado\n",
        "    printf(\"{1, 2, 3, 4, 5} + {10, 20, 30, 40, 50} = {%d, %d, %d, %d, %d}\\n\", c[0], c[1], c[2], c[3], c[4]);\n",
        "\n",
        "    // Se liberan recursos\n",
        "    cudaDeviceReset();\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1, 2, 3, 4, 5} + {10, 20, 30, 40, 50} = {11, 22, 33, 44, 55}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A57hiA_xM7DU"
      },
      "source": [
        "## Ventajas\n",
        "\n",
        "Debido a lo que ya conocemos respecto al funcinamiento y desempeño de *CUDA*, podemos afirmar que este tipo de operaciones (suma de vectores) se realizan de manera más sencilla en un *GPU*.\n",
        "\n",
        "Sean $\\vec{a}=\\{1,2,3,4,5\\}$ y $\\vec{b}=\\{10,20,30,40,50\\}$ entonces $\\left(a_{0},b_{0}\\right)$ se envían al núcleo 0, $\\left(a_{1},b_{1}\\right)$ se envían al núcleo 1, así sucesivamente hasta $\\left(a_{n-1},b_{n-1}\\right)$ se envían al núcleo $n-1$. En caso de que existan más entradas que nucleos, entonces se tendría que esperar a liberar alguno de los núcleos previamente empleados, sin embargo debido a la arquitectura de los *GPU's* sabemos que cada *GPU* posee muchos nucleos, **en las tarjetas más recientes podemos hablar del orden de miles**.\n",
        "\n",
        "De igual manera como se midió el tiempo de ejecución con *OpenMP* o *MPI*, existe forma de medir el tiempo de ejecución de los algoritmos usando *CUDA*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4MYFdgKNDcB"
      },
      "source": [
        "## Desventajas\n",
        "\n",
        "La desventaja más notoria en el algoritmo anterior, es el cuello de botella que se genera tanto al enviar los datos a procesar al *GPU*, como al extraerlos del *GPU*, sin embargo es claro que a mayor cantidad de datos a procesar también se tendría una mayor ganancia en tiempo, lo que se traduce en un menor tiempo de ejecución.\n",
        "\n",
        "Este tipo de características (gran cantidad de datos a procesar) normalmente se encuentran en muchas áreas de las ciencias e ingenierías, por ejemplo ***machine learning***, por mencionar alguna.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WogAVZ_eNKKm"
      },
      "source": [
        "# Operaciones con matrices\n",
        "\n",
        "Una vez que ya se comprendió el desempeño de *CUDA* para la suma de vectores, es sencillo extender su aplicación a operaciones con matrices, por ejemplo la **suma de matrices**.\n",
        "\n",
        "Por lo general cuando se hace uso de modelos matemáticos, estos toman la forma de matriz, uno de los más conocidos son las redes neuronales. Estas redes neuronales se representan por una matriz y en cada entrada de la matriz se tiene una neurona, que a su vez cada neurona puede ser representada por un escalar o un vector o incluso otra matriz.\n",
        "\n",
        "Es por este motivo que las *GPU's* y en particular *CUDA* ha mostrado un excelente desempeño en el proceso de entrenamiento de las redes neuronales, recientemente han surgido dispositivos de computo especifico como los *TPU's* (unidades de procesamiento tensorial).\n",
        "\n",
        "Por este motivo la jerarquía de memoria de *CUDA* se estructura de la siguiente manera.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/memtotal.png?raw=1\" width=\"600\"> \n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9kPwGmoNPbY"
      },
      "source": [
        "## Entendiendo CUDA\n",
        "\n",
        "Queda como ejercicio, realizar la suma de matrices empleando *CUDA*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfteTdToPWyV"
      },
      "source": [
        "# Glosario\n",
        "\n",
        "*Host*: En el contexto de *CUDA* el host es el *CPU* del dispositivo de cómputo en el que se ejecuta el algoritmo.\n",
        "\n",
        "Asíncrono: En computación un evento (proceso) asíncrono es aquel no tiene correspondencia temporal con otro evento. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHU_RuNSPbRQ"
      },
      "source": [
        "# Referencias\n",
        "\n",
        "1. Tolga Soyata: GPU Parallel Program Development Using Cuda.\n",
        "2. https://fisica.cab.cnea.gov.ar/gpgpu/images/clases/clase_1_cuda.pdf\n",
        "3. Dongarra Foster: Source Book of parallel computing."
      ]
    }
  ]
}