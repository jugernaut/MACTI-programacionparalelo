{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUDA_SCP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugernaut/ProgramacionEnParalelo/blob/desarrollo/CUDA/01_CUDA_SCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rYgUDA8Ltm4"
      },
      "source": [
        "<font color=\"Teal\" face=\"Comic Sans MS,arial\">\n",
        "  <h1 align=\"center\"><i>Cuda (Programación con GPU's)</i></h1>\n",
        "  </font>\n",
        "  <font color=\"Black\" face=\"Comic Sans MS,arial\">\n",
        "  <h5 align=\"center\"><i>Profesor: M.en.C. Miguel Angel Pérez León</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Jesús Iván Coss Calderón</i></h5>\n",
        "    <h5 align=\"center\"><i>Ayudante: Mario Arturo Nieto Butron</i></h5>\n",
        "  <h5 align=\"center\"><i>Materia: Seminario de programación en paralelo</i></h5>\n",
        "  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTIY01-ELvbH"
      },
      "source": [
        "# Introducción\n",
        "\n",
        "Ademas de las herramientas previamente vistas para programación en paralelo (*OpenMP* y *MPI*), otra forma de realizar cómputo en paralelo es mediante *CUDA*.\n",
        "\n",
        "*CUDA* debe su nombre al acrónimo en ingles de Arquitectura Unificada de Dispositivos de Computo (*Compute Unified of Device Arquitecture*).\n",
        "\n",
        "De manera similar a *OpenMP* y *MPI*, *CUDA* es un *API* que en conjunto con el lenguaje *C/C++*, permite realizar cómputo en paralelo empleando los *GPU's* que se tengan disponibles.\n",
        "\n",
        "Existen diferentes wrappers (envoltorios) para utilizar *CUDA* en otros lenguajes como *Python*, *Fortran* o incluso *Java*, sin embargo en esta presentación veremos las instrucciones básicas para el lenguaje *C/C++*.\n",
        "\n",
        "Vale la pena mencionar que el desarrollo de este *API* (*CUDA*) es llevado a cabo por la empresa *Nvidia*.\n",
        "\n",
        "Inicialmente los *GPU's* (unidades de procesamiento gráfico) fueron diseñadas para procesamiento de imágenes, sin embargo se ha probado que muestran un gran desempeño no solo en esta área.\n",
        "\n",
        "La forma en la que trabaja *CUDA* es enviando la información a procesar directamente a la tarjeta gráfica, ahí es procesada y finalmente devuelta al *CPU*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/menycore.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/cuda.png?raw=1\" width=\"900\"> \n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyAG-V_3MwrN"
      },
      "source": [
        "# Desempeño\n",
        "\n",
        "El desempeño de *CUDA* difiere un poco de los *API's* revisados con anterioridad en el hecho de que *CUDA* hace uso de la tarjeta gráfica *(GPU)* para el procesamiento en paralelo.\n",
        "\n",
        "Dentro de la tarjeta gráfica, la unidad central de procesamiento, es conocida como *GPU* (*Graphic Procesing Unit*), que como su nombre lo indica, es la unidad de procesamiento gráfico y por lo tanto califica como un dispositivo de cómputo de propósito especifico.\n",
        "\n",
        "Además de los *CPU's* y *GPU's* existen algunos otros tipos de dispositivos de cómputo, como los *TPU's* o los *FPGA's* sin embargo estos 2 últimos escapan a los alcances de este curso.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/gpucpu.jpg?raw=1\" width=\"600\"> \n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6YKyaVxM19I"
      },
      "source": [
        "## ¿Cómo funciona?\n",
        "\n",
        "La forma tradicional de la programación en paralelo empleando *CUDA* es la siguiente: \n",
        "\n",
        "*   En un determinado momento en que se ejecuta un programa, se envía desde la memoria *RAM* la información que sera procesada por los GPU's.\n",
        "*   Una vez que la información se ubica en la tarjeta gráfica se generan los thread (a nivel *GPU*) que se vayan a ocupar.\n",
        "*   Se realizan los cálculos necesarios en paralelo.\n",
        "*   Se devuelve un el resultado del procesamiento al *CPU*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/gpuwork.jpg?raw=1\" width=\"800\"> \n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A57hiA_xM7DU"
      },
      "source": [
        "## Ventajas\n",
        "\n",
        "La principal diferencia entre *MPI*, *OpenMP* y *CUDA*, es que este ultimo realiza todos los cálculos empleando *GPU's* (dispositivos de cómputo de propósito específico) en lugar de los *CPU's* (dispositivos de cómputo de propósito general) que utilizan *MPI* y *OpenMP*.\n",
        "\n",
        "La ventaja más evidente de *CUDA* es la de utilizar *GPU's*, mismos que están **optimizados para operaciones matriciales y vectoriales**.\n",
        "\n",
        "La memoria constante, como su propio nombre indica, se usa para albergar datos que no cambian durante el transcurso de ejecución de un kernel, el uso de memoria constante en lugar de la memoria global pueda reducir considerablemente el tiempo de procesamiento.\n",
        "\n",
        "Se tiene soporte a nivel hardware para operaciones con enteros y con bits.\n",
        "\n",
        "*CUDA* ha mostrado gran desempeño para labores de inteligencia artificial tales como **deep learning y machine learning**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4MYFdgKNDcB"
      },
      "source": [
        "## Desventajas\n",
        "\n",
        "No todo es miel sobre hojuelas al emplear *CUDA* y aquí algunas de sus desventajas:\n",
        "\n",
        "*   La transferencia de información entre el *CPU* y la tarjeta gráfica genera un cuello de botella, revisar **ley de Amdhal**.\n",
        "*   No es posible ejecutar algoritmos recursivos.\n",
        "*   La unidad mínima de bloques debe ser de 32 threads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WogAVZ_eNKKm"
      },
      "source": [
        "# *CUDA*\n",
        "\n",
        "*CUDA* al igual que *OpenMP* y *MPI* es un *API* escrito para el lenguaje *C/C++*, lo que significa que es un conjunto de funciones y directivas que permiten procesar los datos haciendo uso de los *GPU's* disponibles.\n",
        "\n",
        "Debido a la naturaleza de *CUDA* y sobretodo a que es desarrollada por la empresa *Nvidia*, la instalación de *CUDA* requiere unos cuantos pasos adicionales, que de manera resumida se listan a continuación:\n",
        "\n",
        "1.   **Identificar el modelo de la tarjeta gráfica**: este paso es muy importante ya que de esto depende el resto del procedimiento.\n",
        "2.   **Descargar e instalar los controladores**: una vez que se conoce el modelo de la tarjeta gráfica es necesario descargar del sitio de [*Nvidia*](https://www.nvidia.com/Download/Find.aspx?lang=es) los controladores compatibles con el modelo de tarjeta.\n",
        "3.   **Descargar e instalar *CUDA***: con base en la versión de los controladores de Nvidia instalados, es necesario identificar la versión compatible de [*CUDA*](https://developer.nvidia.com/cuda-downloads), descargar e instalar la misma.\n",
        "\n",
        "Una vez que se instaló *CUDA*, ya es posible compilar y ejecutar código escrito mediante este *API*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9kPwGmoNPbY"
      },
      "source": [
        "## Funcionamiento de *CUDA*\n",
        "\n",
        "A diferencia de los *API's* anteriores (*OmpenMP, MPI*) *CUDA* se maneja un poco diferente.\n",
        "\n",
        "En *CUDA* toda sección de código en paralelo se ejecuta mediante un ***Kernel***, es decir Kernel = Función.\n",
        "\n",
        "Un *Kernel* se ejecuta en paralelo mediante *threads* de *GPU*.\n",
        "\n",
        "Cada *Kernel* esta dividido en ***Blocks*** de una, dos o tres dimensiones de *threads* (hilos).\n",
        "\n",
        "Finalmente un ***Grid*** puede ser una arreglo de una, dos o tres dimensiones de *blocks*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/total.png?raw=1\" width=\"600\"> \n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6Z7lCWUN7kY"
      },
      "source": [
        "# Jerarquía de memoria\n",
        "\n",
        "Dadas los elementos del funcionamiento de *CUDA*, se tienen diferentes tipos de memoria y acceso a la misma, veamos.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/memtotal.png?raw=1\" width=\"600\"> \n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UGHISwQOkgt"
      },
      "source": [
        "# *API CUDA*\n",
        "\n",
        "Los identificadores de funciones más importantes que se tienen en *CUDA* son los siguientes.\n",
        "\n",
        "1. __global__: indica que una función es de tipo *kernel*, es decir que sera ejecutada en un dispositivo (*GPU*) y solo puede ser llamada desde el host. Al momento de ser invocada se genera un *grid* de bloques con un número fijo e igual de *threads*.\n",
        "\n",
        "2. __device__: es una función que solo puede ser llamada desde un dispositivo, es decir que esta función solo puede ser invocada desde un *kernel* o desde otra función de tipo *device*. \n",
        "\n",
        "3. __host__: indica que esta función pertenece al host es decir que esta función solo puede ser ejecutada en el *host*. En otras palabras es una función de *C/C++* tradicional.\n",
        "\n",
        "Algunas de las variables reservadas de *CUDA* y que son de gran utilidad son las siguientes:\n",
        "\n",
        "*   **gridDim**: indica las dimensiones del *grid*.\n",
        "*   **blockIdx**: indica el identificador del Bloque de un *grid*.\n",
        "*   **blockDim**: contiene las dimensiones de un *block*.\n",
        "*   **threadIdx**: contiene el identificador del *thread* dentro del *block*.\n",
        "\n",
        "Es importante notar que todas estas variables contienen componentes en $X, Y$ y $Z$. Los *grids* y los *bloques* pueden ser de 1, 2 o 3 dimensiones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LAe7md8O68I"
      },
      "source": [
        "# Compilación y Ejecución\n",
        "\n",
        "Una vez instaladas las herramientas necesarias, solo es necesario usar las banderas (*flags*) correctas al momento de compilar y ejecutar algún programa escrito usando *CUDA*.\n",
        "\n",
        "La manera en la que se compila (revisión sintáctica) y se ejecuta un programa codificado mediante *CUDA*, es muy similar a como se compila y se ejecuta cualquier programa escrito en *C/C++*.\n",
        "\n",
        "Veamos como se compila y ejecuta tanto en *google colab*, como en un equipo local."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bivnzcwAVcOF"
      },
      "source": [
        "*## CUDA* en *Google Colab*\n",
        "\n",
        "Normalmente una vez iniciada la sesión de *google colab*, esta ya cuenta con todo lo necesario para compilar y ejecutar un programa usando *CUDA*, unicamente se tiene que habilitar el uso de los *GPU's*.\n",
        "\n",
        "Para habilitar el uso del *GPU*, es necesario dar click en *Entorno de ejecución -> Cambiar tipo de entorno de ejecución*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/ProgramacionEnParalelo/blob/desarrollo/Imagenes/CUDA/seleccion.png?raw=1\" width=\"400\"> \n",
        "</center>\n",
        "\n",
        "Finalmente seleccionar *GPU*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/ProgramacionEnParalelo/blob/desarrollo/Imagenes/CUDA/gpu.png?raw=1\" width=\"500\"> \n",
        "</center>\n",
        "\n",
        "Una vez que se reinicio el entorno de ejecución, lo siguiente es instalar el *plugin* neceario para compilar y ejecutar código de *CUDA* en *google colab*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyC-bWQidzPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1b55bf-aeb0-40fc-824b-073afc8debbd"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-iz0bp_x2\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-iz0bp_x2\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4306 sha256=1c050e115f79ff307536a3c8504862edf69f5a326a5ce73fddd5c7e772479b47\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xfernhuh/wheels/c5/2b/c0/87008e795a14bbcdfc7c846a00d06981916331eb980b6c8bdf\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgxkaJVJfnB9"
      },
      "source": [
        "Ya con el *plugin* instalado, se carga este plugin en la sesión actual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYfCwyYTdK_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ec71d8-1e2c-48ca-d042-14314595fcf9"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwUQT_r0fYmh"
      },
      "source": [
        "Antes de escribir el código solo resta agregar *%%cu* en el encabezado de cualquier celda de código que contenga *CUDA* para que esta sea ejecutada como si fuera cualquier celda de código común."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxP4ZZ17dDGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c8cf72-5562-4b27-cc67-5023bc4c047c"
      },
      "source": [
        "# necesario para compilar codigo de CUDA\n",
        "%%cu\n",
        "\n",
        "// Biblioteca de entrada/salida\n",
        "#include <stdio.h>\n",
        "// Definicion de un kernel\n",
        "__global__ void helloFromGPU(void){\n",
        "        printf(\"Hola mundo desde el GPU! threadIdx.x=%d\\n\", threadIdx.x);\n",
        "}\n",
        "\n",
        "// Funcion principal\n",
        "int main(void){\n",
        "   // Hola desde el CPU\n",
        "   printf(\"Hola mundo desde el CPU!\\n\");\n",
        "\n",
        "   // Llamada al kernel que se ejecuta en el GPU\n",
        "   helloFromGPU <<<1, 10>>>();\n",
        "   // Se liberan los recursos utilizados\n",
        "   cudaDeviceReset();\n",
        "   return(0);\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hola mundo desde el CPU!\n",
            "Hola mundo desde el GPU! threadIdx.x=0\n",
            "Hola mundo desde el GPU! threadIdx.x=1\n",
            "Hola mundo desde el GPU! threadIdx.x=2\n",
            "Hola mundo desde el GPU! threadIdx.x=3\n",
            "Hola mundo desde el GPU! threadIdx.x=4\n",
            "Hola mundo desde el GPU! threadIdx.x=5\n",
            "Hola mundo desde el GPU! threadIdx.x=6\n",
            "Hola mundo desde el GPU! threadIdx.x=7\n",
            "Hola mundo desde el GPU! threadIdx.x=8\n",
            "Hola mundo desde el GPU! threadIdx.x=9\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5UDZ9GxPAhX"
      },
      "source": [
        "## *CUDA* en equipo local\n",
        "\n",
        "Una vez instalado el *API* de *CUDA* para el respectivo hardware la forma de compilar y ejecutar código de *CUDA* es muy similar al lenguaje *C/C++*.\n",
        "\n",
        "Supongamos que ya se cuenta con el código fuente de \"hola mundo\" para *CUDA* (helloCUDA.cu)\n",
        "\n",
        "*   Para compilar: *\\$nvcc helloCUDA.cu -o (helloCUDA.o) codigo*\n",
        "\n",
        "El comando anterior compila (y en caso de no haber errores) y genera un archivo ejecutable (binario) que puede ser ejecutado de la siguiente manera:\n",
        "\n",
        "*   Para ejecutar: *\\$./codigo*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfteTdToPWyV"
      },
      "source": [
        "# Glosario\n",
        "\n",
        "Dispositivo: En el contexto de *CUDA* un dispositivo hace referencia a un *GPU*.\n",
        "\n",
        "Asíncrono: En computación un evento (proceso) asíncrono es aquel no tiene correspondencia temporal con otro evento. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHU_RuNSPbRQ"
      },
      "source": [
        "# Referencias\n",
        "\n",
        "1. Tolga Soyata: GPU Parallel Program Development Using Cuda.\n",
        "2. https://fisica.cab.cnea.gov.ar/gpgpu/images/clases/clase_1_cuda.pdf\n",
        "3. Dongarra Foster: Source Book of parallel computing."
      ]
    }
  ]
}