{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jugernaut/ProgramacionEnParalelo/blob/main/RedesNeuronales/04_SOM_SCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKcKm7OJbfaC"
   },
   "source": [
    "<font color=\"Teal\" face=\"Comic Sans MS,arial\">\n",
    "  <h1 align=\"center\"><i>SOM</i></h1>\n",
    "  </font>\n",
    "  <font color=\"Black\" face=\"Comic Sans MS,arial\">\n",
    "  <h5 align=\"center\"><i>Profesor: M. en C. Miguel Angel Pérez León</i></h5>\n",
    "  <h5 align=\"center\"><i>Ayudante: Lucía Martínez Rivas</i></h5>\n",
    "  <h5 align=\"center\"><i>Ayudante: Erick Jesús Rios Gonzalez</i></h5>\n",
    "  <h5 align=\"center\"><i>Materia: Seminario de programación en paralelo</i></h5>\n",
    "  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pL3l51BpO_hL"
   },
   "source": [
    "## Introducción\n",
    "\n",
    "El concepto de **norma** es de gran importancia en el contexto de los espacios vectoriales, tiene multiples aplicaciones y una de ellas es para definir metricas en dichos espacios para poder medir distancias entre los elementos que pertenecen a estos espacios vectoriales.\n",
    "\n",
    "La distancia entre un vector y el origen o entre una matriz y otra puede tener multiples interpretaciones, en este documento verémos como el concepto de norma se aplica al campo de la inteligencia artificial y en particular en las redes neuronales.\n",
    "\n",
    "Las redes neuronales y el aprendizaje de máquina es de las áreas de las ciencias que han tenido más desarrollo en las últimas epocas.\n",
    "\n",
    "Y una de las áreas en las que mayor aplicación han tenido estas disciplinas es en la **clasificicación automatizada**.\n",
    "\n",
    "Supongamos que nos interesa clasificar colores y dado que los colores se pueden ver como vectores con 3 componentes entonces podemos establecer distancias entre estos vectores para determinar que tan \"similares\" o \"distintos\" son unos de otros.\n",
    "\n",
    "A partir de este concepto de \"distancia\" (norma en un espacio vectorial) podemos comenzar a clasificar cualquier cosa que pueda ser caracterizada por un vector.\n",
    "\n",
    "Para una idea más clara de lo antes mencionado puedes ver este [video](https://youtu.be/h5cXKVf0ogQ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUR6fBxlqMgC"
   },
   "source": [
    "## Mapa auto organizado (*SOM*)\n",
    "\n",
    "Un mapa auto organizado o *SOM* por sus siglas en inglés (*self-organized map*) es una de las redes neuronales más sencillas y fáciles de implementar pero no por eso es un algoritmo que no tenga aplicación actualmente.\n",
    "\n",
    "Este tipo de red neuronal fue creado en la decada de los 80's por el por el finlandés **Teuvo Kohonen** y se basa en modelos matemáticos de **Alan Turing**.\n",
    "\n",
    "La idea detrás de este algoritmo es muy sencilla y se describe de manera breve a continuación:\n",
    "\n",
    "*   Comenzamos con una red o mapa (matriz) de vectores o incluso de matrices en la cual todas las neuronas o entradas del mapa contienen valores aleatorios.\n",
    "*   Por cada elemento en la lista de entrenamiento, se evalua la norma (distancia) de este elemento contra cada neurona en la red.\n",
    "*   Tomamos aquella neurona cuya norma haya sido la menor y modificamos los valores de las neuronas vecinas para que se parezcan un poco al vector evaluado en esa iteración.\n",
    "*   Se repite este proceso hasta terminar las iteraciones ó en caso de que la norma de la red actual y la red anterior no difiere mucho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQdgCiNDrBRg"
   },
   "source": [
    "### Formalización del algoritmo\n",
    "\n",
    "Para dar un formalización de este algoritmo es necesario definir un conjunto de variables que son usadas durante el proceso de entrenamiento y clasificación de la red nueronal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGzJ_yoeEYGI"
   },
   "source": [
    "#### Variables\n",
    "\n",
    "*   $s$ es la iteración actual.\n",
    "*   $\\lambda$ cantidad de ciclos de entrenamiento o epocas.\n",
    "*   $t$ es el índice del vector de entrada en el conjunto de datos de entrada $D$.\n",
    "*   $D(t)$ es un vector de entrada de índice $t$ del conjunto de datos de entrada $D$.\n",
    "*   $v$ es el índice de una neurona en el mapa.\n",
    "*   $W_v$ es el vector de pesos de la neurona v.\n",
    "*   $u$ es el índice de la neurona cuya norma es la menor con respecto de $W_v$\n",
    "*   $\\Theta (u,v,s)$ es la función de vecindad que determina cuáles neuronas serán modificadas.\n",
    "*   $\\alpha (s)$ es una función que restringe el aprendizaje conforme avanzan las iteraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNvm0iYHEaVH"
   },
   "source": [
    "#### Algoritmo\n",
    "\n",
    "1.   Hacer un mapa (red) de neuronas con vectores de pesos aleatorios.\n",
    "2.   Tomar un vector de entrada $D(t)$.\n",
    "\n",
    ">1.   Iterar por cada neurona del mapa.\n",
    "\n",
    ">>1.   Calcular la distancia entre el vector de entrada y los vectores de pesos de las neuronas del mapa.\n",
    "2.   Mantener la neurona que ha tenido la menor distancia (norma), esta neurona será el best matching unit (BMU).\n",
    "\n",
    ">2.   Actualizar las neuronas en la vecindad del BMU.\n",
    "\n",
    ">> 1.   $W_{v}\\left(s+1\\right)=W_{v}\\left(s\\right)+\\Theta\\left(u,v,s\\right)\\alpha\\left(s\\right)\\left(D\\left(t\\right)-W_{v}\\left(s\\right)\\right)$\n",
    "\n",
    "3.   Incrementar $s$ y volver al paso 2, mientras $s<\\lambda$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2t_BRrcZMhm1"
   },
   "source": [
    "## *Tensor Flow*\n",
    "\n",
    "*Tensor Flow* es un *API* desarrollado por Google que tiene la finalidad de facilitar la implementación de redes neuronales.\n",
    "\n",
    "Se usa en multiples aplicaciones de Google como su **reconocimiento automatizado de imagenes** en el buscador o en el **reconocimiento de patrones de voz** para su asistente virtual.\n",
    "\n",
    "Existen versiones de *Tensor Flow* optimizadas para hacer uso de *GPU's* (unidades de procesamiento grafico) o incluso de *TPU's* (unidades de procesamiento tensorial).\n",
    "\n",
    "Actualmente se encuentra en su versión 2.3 sin embargo para el ejemplo que veremos haremos uso de la versión 1.1.\n",
    "\n",
    "Su funcionamiento es muy intuitivo y se recomienda revisar su [documentación](https://www.tensorflow.org/api_docs) para una mayor comprensión del código mostrado a continuación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdVDqeHfJZW1"
   },
   "source": [
    "## Implementación\n",
    "\n",
    "Para tener una mejor organización del algoritmo y mediante el paradigma orientado a objetos, se crea la **clase SOM** que encapsula las acciones que podemos llevar a cabo con un *SOM*.\n",
    "\n",
    "El código ha sido comentado para su mejor comprensión pero de manera general implementa el algoritmo descrito previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.26.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (13.8.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85Aws8yJGKXm",
    "outputId": "22e09d46-9993-47e9-c8fc-e2789ca3b126"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow==1.1\n",
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    " \n",
    "class SOM(object):\n",
    "    \"\"\"\n",
    "    Clase que representa una red neuronal tipo SOM.\n",
    "    \"\"\"\n",
    " \n",
    "    #Para revisar si la red ya ha sido entrenada\n",
    "    _trained = False\n",
    " \n",
    "    def __init__(self, m, n, dim, n_iterations=100, alpha=None, sigma=None):\n",
    "        \"\"\"\n",
    "        Constructor que toma como parametros los valores descritos en el\n",
    "        algoritmo SOM. Genera un mapa de m renglones por n columnas y se entrenara\n",
    "        con n_iterations\n",
    "        \"\"\"\n",
    " \n",
    "        #Se inicializan variables que seran usadas a lo largo del coidgo\n",
    "        self._m = m\n",
    "        self._n = n\n",
    "        if alpha is None:\n",
    "            alpha = 0.3\n",
    "        else:\n",
    "            alpha = float(alpha)\n",
    "        if sigma is None:\n",
    "            sigma = max(m, n) / 2.0\n",
    "        else:\n",
    "            sigma = float(sigma)\n",
    "        self._n_iterations = abs(int(n_iterations))\n",
    " \n",
    "        '''SE NECESITA UNA GRAFICA (PLANO), hay una grafica\n",
    "        por default pero la guardamos en _graph'''\n",
    "        self._graph = tf.Graph()\n",
    " \n",
    "        '''SE CREAN LOS ELEMENTOS NECESARIOS EN LA GRAFICA'''\n",
    "        with self._graph.as_default():\n",
    "            '''SE CREAN TODAS LAS NEURONAS CON tf.Variable, son m*n\n",
    "            neuronas con dim pesos, que seran comparados con los pesos\n",
    "            de la entrada y la que tenga la menor distancia sera la\n",
    "            neurona ganadora. Antes de iniciar el entrenamiento, hay\n",
    "            hay que inicializar TODAS las variables'''\n",
    "            \n",
    "            '''Lista de pesos de los vectores de la red neuronal'''\n",
    "            self._weightage_vects = tf.Variable(tf.random_normal(\n",
    "                [m*n, dim]))\n",
    " \n",
    "            '''Lista de 600 entradas, y cada entrada representa una\n",
    "            coordenada en la cual se encuentra cada neurona'''\n",
    "            self._location_vects = tf.constant(np.array(\n",
    "                list(self._neuron_locations(m, n))))\n",
    " \n",
    "            '''self._vect_input es un placeholder de tamano dim, ya que\n",
    "            es el objeto que sera alimentado con el vector de entrada y\n",
    "            a su vez este sera comparado con los pesos de cada neurona.\n",
    "            Esto es asi por el framework que da tensorflow'''\n",
    "            self._vect_input = tf.placeholder(\"float\", [dim])\n",
    "            \n",
    "            '''Lo mismo sucede con esta variable, la diferencia es que en\n",
    "            este punto aun no se sabe cuantas iteraciones (epocas) seran\n",
    "            necesarias, asi que se deja en cero.'''\n",
    "            self._iter_input = tf.placeholder(\"float\")\n",
    " \n",
    "            '''Devuelve el indice con el menor valor, es decir la neurona mas cercana.'''\n",
    "            bmu_index = tf.argmin(tf.sqrt(tf.reduce_sum(\n",
    "                tf.pow(tf.subtract(self._weightage_vects, tf.stack(\n",
    "                    [self._vect_input for i in range(m*n)])), 2), 1)),\n",
    "                                  0)\n",
    " \n",
    "            '''Variable que guarda el indice y un espacio para el sus\n",
    "            coordenada'''\n",
    "            slice_input = tf.pad(tf.reshape(bmu_index, [1]),\n",
    "                                 np.array([[0, 1]]))\n",
    "            bmu_loc = tf.reshape(tf.slice(self._location_vects, slice_input,\n",
    "                                          tf.constant(np.array([1, 2]))),\n",
    "                                 [2])\n",
    " \n",
    "            '''Valores necesario para actualizar los pesos de las neuronas\n",
    "            de acuerdo a la iteracion (epoca)'''\n",
    "            learning_rate_op = tf.subtract(1.0, tf.div(self._iter_input,\n",
    "                                                  self._n_iterations))\n",
    "            _alpha_op = tf.multiply(alpha, learning_rate_op)\n",
    "            _sigma_op = tf.multiply(sigma, learning_rate_op)\n",
    " \n",
    "            '''Calcula las distancias al cuadrado por cada neurona con respecto\n",
    "            a la neurona GANADORA (BMU). De tal manera que estos valores\n",
    "            puedan ser empleados para actualizar los pesos de los vecinos'''\n",
    "            bmu_distance_squares = tf.reduce_sum(tf.pow(tf.subtract(\n",
    "                self._location_vects, tf.stack(\n",
    "                    [bmu_loc for i in range(m*n)])), 2), 1)\n",
    "            neighbourhood_func = tf.exp(tf.negative(tf.div(tf.cast(\n",
    "                bmu_distance_squares, \"float32\"), tf.pow(_sigma_op, 2))))\n",
    "            learning_rate_op = tf.multiply(_alpha_op, neighbourhood_func)\n",
    " \n",
    "            '''Tasa de aprendizaje para actualizar los pesos de las neuronas'''\n",
    "            learning_rate_multiplier = tf.stack([tf.tile(tf.slice(\n",
    "                learning_rate_op, np.array([i]), np.array([1])), [dim])\n",
    "                                               for i in range(m*n)])\n",
    "            weightage_delta = tf.multiply(\n",
    "                learning_rate_multiplier,\n",
    "                tf.subtract(tf.stack([self._vect_input for i in range(m*n)]),\n",
    "                       self._weightage_vects)) \n",
    "            \n",
    "            '''Actualiza todos los pesos de las neuronas de acuerdo a los\n",
    "            parametros calculados previamente'''                                        \n",
    "            new_weightages_op = tf.add(self._weightage_vects,\n",
    "                                       weightage_delta)\n",
    "            \n",
    "            '''Se guarda la ultima operacion realizada en la SOM, ya que\n",
    "            esta operacion sera la que se ejecute y a su vez ejecuta todas\n",
    "            las operaciones previar al llamar a sess.run()'''\n",
    "            self._training_op = tf.assign(self._weightage_vects,\n",
    "                                          new_weightages_op)                                       \n",
    " \n",
    "            '''En tensorflow todo debe ocurrir dentro de una sesion, es por\n",
    "            este motivo que se guarda la sesion'''\n",
    "            self._sess = tf.Session()\n",
    " \n",
    "            '''Forma en la tensorflow inicializa sus variables antes de ser\n",
    "            utilizadas'''\n",
    "            init_op = tf.initialize_all_variables()\n",
    "            self._sess.run(init_op)\n",
    "            \n",
    "            '''centroid_grid es un mapa de bits en el cual se guardan los\n",
    "            valores de las neuronas. Es de tamano m, por que para cada renglon\n",
    "            se tienen n neuronas y sus respectivos valores. '''\n",
    "            centroid_grid = [[] for i in range(self._m)]\n",
    "            self._weightages = list(self._sess.run(self._weightage_vects))\n",
    "            self._locations = list(self._sess.run(self._location_vects))\n",
    "    \n",
    "            '''Con este for, se accede a cada neurona por posicion y se guarda\n",
    "            en centroid_grid sus pesos. El resultado es un mapa de bits que puede\n",
    "            ser facilmente graficado por matplotlib. Es el mapa incial (SIN ENTRENAR)'''\n",
    "            for i, loc in enumerate(self._locations):\n",
    "                centroid_grid[loc[0]].append(self._weightages[i])\n",
    "            self._mapa_inicial = centroid_grid\n",
    " \n",
    "    def _neuron_locations(self, m, n):\n",
    "        '''Yield regresa un generador flojo, y hasta que es necesario\n",
    "        se evalua. Esto se hace para que no haya informacion no necesaria\n",
    "        en memoria. En el constructor el resultado de esta funcion se\n",
    "        mete en una lista para que sea accesible de inmediato'''\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                yield np.array([i, j])\n",
    " \n",
    "    def train(self, input_vects):\n",
    "        '''Para cada iteracion (epoca) se realiza el entrenamiento'''\n",
    "        for iter_no in range(self._n_iterations):\n",
    "            actual = self._sess.run(tf.norm(self._weightage_vects))\n",
    "            #Se entrena con cada vector uno por uno\n",
    "            for input_vect in input_vects:\n",
    "                self._sess.run(self._training_op,\n",
    "                               feed_dict={self._vect_input: input_vect,\n",
    "                                          self._iter_input: iter_no})\n",
    "            siguiente = self._sess.run(tf.norm(self._weightage_vects))\n",
    "            '''Si la norma del mapa actual no varia mucho con respecto\n",
    "            al siguiente, se rompe el ciclo de las epocas'''\n",
    "            if abs(siguiente - actual) <= 0.000001:\n",
    "                break\n",
    " \n",
    "        '''centroid_grid es un mapa de bits en el cual se guardan los\n",
    "            valores de las neuronas. Es de tamano m, por que para cada renglon\n",
    "            se tienen n neuronas y sus respectivos valores. '''\n",
    "        centroid_grid = [[] for i in range(self._m)]\n",
    "        self._weightages = list(self._sess.run(self._weightage_vects))\n",
    "        self._locations = list(self._sess.run(self._location_vects))\n",
    "        \n",
    "        '''Con este for, se accede a cada neurona por posicion y se guarda\n",
    "            en centroid_grid sus pesos. El resultado es un mapa de bits que puede\n",
    "            ser facilmente graficado por matplotlib. En este punto la red ya esta entrenada.'''\n",
    "        for i, loc in enumerate(self._locations):\n",
    "            centroid_grid[loc[0]].append(self._weightages[i])\n",
    "        self._centroid_grid = centroid_grid\n",
    " \n",
    "        '''En este punto la red ya esta entrenada.'''\n",
    "        self._trained = True\n",
    " \n",
    "    def get_centroids(self):\n",
    "        # Solo devuelve los centroides para que puendan ser graficados\n",
    "        if not self._trained:\n",
    "            raise ValueError(\"La red aun no ha sido entrenada\")\n",
    "        return self._centroid_grid\n",
    " \n",
    "    def map_vects(self, input_vects):\n",
    "        '''to_return es la lista que contiene las coordenadas (x,y) de la\n",
    "        neurona que mas se parece a cada una de las entradas de input_vects\n",
    "        en el mismo orden'''\n",
    " \n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    " \n",
    "        to_return = []\n",
    "        for vect in input_vects:\n",
    "            min_index = min([i for i in range(len(self._weightages))],\n",
    "                            key=lambda x: np.linalg.norm(vect-\n",
    "                                                         self._weightages[x]))\n",
    "            to_return.append(self._locations[min_index])\n",
    " \n",
    "        return to_return\n",
    "    \n",
    "    def map_vect(self, vect):\n",
    "        '''\n",
    "        Mapea un solo vector y devuelve la clasificacion vista como\n",
    "        un indice relacionado a la coordenada (x,y) de la neurona\n",
    "        '''\n",
    "\n",
    "        min_index = min([i for i in range(len(self._weightages))],\n",
    "                        key=lambda x: np.linalg.norm(\n",
    "                            vect - self._weightages[x]))\n",
    "        pos2D = self._locations[min_index]\n",
    "        # polinomio de direccionamiento de la neurona\n",
    "        #return pos2D[0]*self._m + pos2D[1], pos2D\n",
    "        return (pos2D[1], pos2D[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZABpYxgBKIpq"
   },
   "source": [
    "### Métodos relevantes\n",
    "\n",
    "Todo el código escrito para la clase *SOM* tiene un objetivo en específico sin embargo para fines prácticos nos enfocaremos en un par de métodos que tienen relación directa con el concepto de norma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYXbD2XBKhTb"
   },
   "source": [
    "#### Método *train*\n",
    "\n",
    "Este método se encarga de realizar el ciclo de entrenamiento de la red neuronal, en este método se compara cada uno de los vectores de entrada contra todas las neuronas de la red.\n",
    "\n",
    "Una vez que se localiza la *BMU* se actualizan los pesos de las neuronas vecinas empleando la funcion de vecindad y la funcion de aprendizaje.\n",
    "\n",
    "El entrenamiento concluye una vez que han terminado las iteraciones o epocas, aunque también se concluye el entrenamiento si las redes en la iteración actual y en la siguiente no difieren demasiado y eso lo sabemos **calculando la norma a ambas redes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHbfmowIXru0"
   },
   "source": [
    "#### Método *map_vect*\n",
    "\n",
    "Este método es el encargado de recibir un vector y devolvernos la neurona (en forma de coordenada dentro del mapa) cuya distancia (empleando la norma) sea la menor, eso significa que esa neurona es la más parecida al vector de entrada, en otras palabras obtenemos la clasificación del vector que le mostramos a la red.\n",
    "\n",
    "Si conocemos el nombre o identificador de la neurona que se parece al vector de entrada podemos asignarle esa clasificación al vector de entrada.\n",
    "\n",
    "En esta implementación se usan colores, de manera que al mostrar un color en forma de vector (*R, G, B*) esta red nos indica a que color se parece mas un vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_Ym3FtKRP1t"
   },
   "source": [
    "## Ejecución\n",
    "\n",
    "Se define una lista de colores en formato *RGB (red blue green)* para entrenar a la red neuronal. De igual manera se crea una lista con los nombres de los colores en la lista de colores.\n",
    "\n",
    "Se genera un objeto de tipo *SOM* con sus respectivos parametros.\n",
    "\n",
    "Mostramos la red inicial con colores aleatorios.\n",
    "\n",
    "Se entrena la red de acuerdo a la descripción del algoritmo y a las restricciones del mismo.\n",
    "\n",
    "Posteriormente se muestra la red entrenada.\n",
    "\n",
    "Finalmente se le muestra un color en forma de vector a la red neuronal ya entrenada y esta devuelve un par de coordenadas, lo que en otras palabras significa que nos vevuelve las coordenadas de la neurona que se parece más al color mostrado. Es decir, nos devuelve una clasificación del color mostrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting minisom\n",
      "  Downloading MiniSom-2.3.3.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: minisom\n",
      "  Building wheel for minisom (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for minisom: filename=MiniSom-2.3.3-py3-none-any.whl size=11704 sha256=367c48eeb590522d090bf545c68443ac123593f33a3e2a8a99886d75b11aca2f\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/c8/29/ba/ee521d9a6af783a1e1efb1831fe4afdf526613849b3f58175b\n",
      "Successfully built minisom\n",
      "Installing collected packages: minisom\n",
      "Successfully installed minisom-2.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install minisom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "oYjY41_oGwmq",
    "outputId": "55b99b82-f568-4ae5-88b1-c7fd64a99d6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/minisom.py:164: UserWarning: Warning: sigma might be too high for the dimension of the map.\n",
      "  warn('Warning: sigma might be too high ' +\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MiniSom' object has no attribute '_mapa_inicial'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m som \u001b[38;5;241m=\u001b[39m MiniSom(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m400\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Se muestra el mapa inicial\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m mapa_inicial \u001b[38;5;241m=\u001b[39m \u001b[43msom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mapa_inicial\u001b[49m\n\u001b[1;32m     34\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(mapa_inicial)\n\u001b[1;32m     35\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRed Neuronal Inicial\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MiniSom' object has no attribute '_mapa_inicial'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from minisom import MiniSom\n",
    "\n",
    "# Vectores de entrenamiento RGBcolors\n",
    "colores = np.array(\n",
    "     [[0., 0., 0.],\n",
    "      [0., 0., 1.],\n",
    "      [0., 0., 0.5],\n",
    "      [0.125, 0.529, 1.0],\n",
    "      [0.33, 0.4, 0.67],\n",
    "      [0.6, 0.5, 1.0],\n",
    "      [0., 1., 0.],\n",
    "      [1., 0., 0.],\n",
    "      [0., 1., 1.],\n",
    "      [1., 0., 1.],\n",
    "      [1., 1., 0.],\n",
    "      [1., 1., 1.],\n",
    "      [.33, .33, .33],\n",
    "      [.5, .5, .5],\n",
    "      [.66, .66, .66]])\n",
    "nombres_colores = \\\n",
    "    ['negro', 'azul', 'azul marino', 'azul cielo',\n",
    "     'gris azulado', 'lila', 'verde', 'rojo',\n",
    "     'cyan', 'violeta', 'amarillo', 'blanco',\n",
    "     'gris obscuro', 'gris medio', 'gris claro']\n",
    " \n",
    "# Creamos un SOM de 20x30, cada entrada de la red\n",
    "# es un vector de tamano 3 (R,G,B) y se entrena 400 veces\n",
    "som = MiniSom(20, 30, 3, 400)\n",
    "\n",
    "# Se muestra el mapa inicial\n",
    "mapa_inicial = som._mapa_inicial\n",
    "plt.imshow(mapa_inicial)\n",
    "plt.title('Red Neuronal Inicial')\n",
    "plt.show()\n",
    "\n",
    "# Se entrena la red con un conjunto de colores\n",
    "som.train(colores)\n",
    " \n",
    "# Obtenemos el SOM ya entrenado\n",
    "red_entrenada = som.get_centroids()\n",
    " \n",
    "# Contiene la lista de coordenadas de los correspondientes colores\n",
    "mapped = som.map_vects(colores)\n",
    " \n",
    "# Grafica\n",
    "plt.imshow(red_entrenada)\n",
    "plt.title('Red Neuronal Entrenada')\n",
    "for i, m in enumerate(mapped):\n",
    "    plt.text(m[1], m[0], nombres_colores[i], ha='center', va='center',\n",
    "             bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
    "plt.show()\n",
    "\n",
    "# Se le muestra un color en forma de vector, en este caso azul cielo\n",
    "# para que indique a que neurona se parece mas\n",
    "print(\"El color azul cielo se encuentra en la neurona \"+str(som.map_vect([0.125, 0.529, 1.0])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nKaftH_QgXy"
   },
   "source": [
    "### Mostrando más colores\n",
    "\n",
    "Una vez que la red (mapa) ya fue entrenada, es posible mostrarle cualquier color en forma de vector (*R,G,B*) y la red nos mostrará que clasificación le corresponde, por ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCR8gKF2Q70u",
    "outputId": "0cf24ca1-4771-4b5e-c6ed-937bcdc1ecad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El color amarillo se encuentra en la neurona (29, 19)\n"
     ]
    }
   ],
   "source": [
    "print(\"El color amarillo se encuentra en la neurona \"+str(som.map_vect([1., 1., 0.])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd-B7cS3caHl"
   },
   "source": [
    "## Extendiendo este modelo\n",
    "\n",
    "La implementación en este documento se realizo con colores, ya que facilitan la comprensión del funcionamiento del algoritmo en general.\n",
    "\n",
    "Sin ambargo esta red neuronal puede ser aplicada a cualquier espacio vectorial, en otras palabras, este algoritmo puede ser aplicado a cualquier objeto que podamos representar en forma de vector o matriz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOr5r_Zfdzg4"
   },
   "source": [
    "### Clasificación de Documentos o Imágenes\n",
    "\n",
    "Para clasificar documentos el algoritmo es exactamente el mismo, lo único que cambia es que tenemos que obtener un **vector caracteristico** para los documentos que nos interes clasificar. Este vector caracteristico se puede obtener de formas muy variadas y una de ellas es **contando la frecuencia de las plabras** que aparecen en dicho documento.\n",
    "\n",
    "Respecto a la clasificación de imágenes, una imagen finalmente es un **mapa de pixeles**, mismo que puede ser representado por un vector de vectores, es decir un **vector de colores**, lo que en si ya un vector caracteristico de dicha imágen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAh_UjLcaKr7"
   },
   "source": [
    "## Referencias\n",
    "\n",
    "*   https://codesachin.wordpress.com/2015/11/28/self-organizing-maps-with-googles-tensorflow/\n",
    "*   http://www.saedsayad.com/clustering_som.htm\n",
    "*   https://www.tensorflow.org/install\n",
    "*   https://relopezbriega.github.io/blog/2016/06/05/tensorflow-y-redes-neuronales/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "04_SOM_SCP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
