{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugernaut/MACTI-programacionparalelo/blob/main/03_CUDA/01_CUDA_SCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rYgUDA8Ltm4"
      },
      "source": [
        "<font color=\"Teal\" face=\"Comic Sans MS,arial\">\n",
        "  <h1 align=\"center\"><i>Cuda (Programación con GPU's)</i></h1>\n",
        "  </font>\n",
        "  <font color=\"Black\" face=\"Comic Sans MS,arial\">\n",
        "  <h5 align=\"center\"><i>Profesor: M. en C. Miguel Angel Pérez León</i></h5>\n",
        "  <h5 align=\"center\"><i>Ayudante: Lucía Martínez Rivas</i></h5>\n",
        "  <h5 align=\"center\"><i>Ayudante: Erick Jesús Rios Gonzalez</i></h5>\n",
        "  <h5 align=\"center\"><i>Materia: Seminario de programación en paralelo</i></h5>\n",
        "  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTIY01-ELvbH"
      },
      "source": [
        "# Introducción\n",
        "\n",
        "Ademas de las herramientas previamente vistas para programación en paralelo (*OpenMP* y *MPI*), otra forma de realizar cómputo en paralelo es mediante *CUDA*.\n",
        "\n",
        "*CUDA* debe su nombre al acrónimo en ingles de Arquitectura Unificada de Dispositivos de Computo (*Compute Unified of Device Arquitecture*).\n",
        "\n",
        "De manera similar a *OpenMP* y *MPI*, *CUDA* es un *API* que en conjunto con el lenguaje *C/C++*, permite realizar cómputo en paralelo empleando los *GPU's* que se tengan disponibles.\n",
        "\n",
        "Existen diferentes wrappers (envoltorios) para utilizar *CUDA* en otros lenguajes como *Python*, *Fortran* o incluso *Java*, sin embargo en esta presentación veremos las instrucciones básicas para el lenguaje *C/C++*.\n",
        "\n",
        "Vale la pena mencionar que el desarrollo de este *API* (*CUDA*) es llevado a cabo por la empresa *Nvidia*.\n",
        "\n",
        "Inicialmente los *GPU's* (unidades de procesamiento gráfico) fueron diseñadas para procesamiento de imágenes, sin embargo se ha probado que muestran un gran desempeño no solo en esta área.\n",
        "\n",
        "La forma en la que trabaja *CUDA* es enviando la información a procesar directamente a la tarjeta gráfica, ahí es procesada y finalmente devuelta al *CPU*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/menycore.png?raw=1\" width=\"600\">\n",
        "</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/cuda.png?raw=1\" width=\"900\">\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyAG-V_3MwrN"
      },
      "source": [
        "# Desempeño\n",
        "\n",
        "El desempeño de *CUDA* difiere un poco de los *API's* revisados con anterioridad en el hecho de que *CUDA* hace uso de la tarjeta gráfica *(GPU)* para el procesamiento en paralelo.\n",
        "\n",
        "Dentro de la tarjeta gráfica, la unidad central de procesamiento, es conocida como *GPU* (*Graphic Procesing Unit*), que como su nombre lo indica, es la unidad de procesamiento gráfico y por lo tanto califica como un dispositivo de cómputo de propósito especifico.\n",
        "\n",
        "Además de los *CPU's* y *GPU's* existen algunos otros tipos de dispositivos de cómputo, como los *TPU's* o los *FPGA's* sin embargo estos 2 últimos escapan a los alcances de este curso.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/gpucpu.jpg?raw=1\" width=\"600\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6YKyaVxM19I"
      },
      "source": [
        "## ¿Cómo funciona?\n",
        "\n",
        "La forma tradicional de la programación en paralelo empleando *CUDA* es la siguiente:\n",
        "\n",
        "*   En un determinado momento en que se ejecuta un programa, se envía desde la memoria *RAM* la información que sera procesada por los GPU's.\n",
        "*   Una vez que la información se ubica en la tarjeta gráfica se generan los thread (a nivel *GPU*) que se vayan a ocupar.\n",
        "*   Se realizan los cálculos necesarios en paralelo.\n",
        "*   Se devuelve un el resultado del procesamiento al *CPU*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/gpuwork.jpg?raw=1\" width=\"800\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A57hiA_xM7DU"
      },
      "source": [
        "## Ventajas\n",
        "\n",
        "La principal diferencia entre *MPI*, *OpenMP* y *CUDA*, es que este ultimo realiza todos los cálculos empleando *GPU's* (dispositivos de cómputo de propósito específico) en lugar de los *CPU's* (dispositivos de cómputo de propósito general) que utilizan *MPI* y *OpenMP*.\n",
        "\n",
        "La ventaja más evidente de *CUDA* es la de utilizar *GPU's*, mismos que están **optimizados para operaciones matriciales y vectoriales**.\n",
        "\n",
        "La memoria constante, como su propio nombre indica, se usa para albergar datos que no cambian durante el transcurso de ejecución de un *Kernel* (función que se ejecuta mediante *GPU*), el uso de memoria constante en lugar de la memoria global puede reducir considerablemente el tiempo de procesamiento.\n",
        "\n",
        "Se tiene soporte a nivel hardware para **operaciones con enteros y con bits**.\n",
        "\n",
        "*CUDA* ha mostrado gran desempeño para labores de inteligencia artificial tales como **deep learning y machine learning**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4MYFdgKNDcB"
      },
      "source": [
        "## Desventajas\n",
        "\n",
        "No todo es miel sobre hojuelas al emplear *CUDA* y aquí algunas de sus desventajas:\n",
        "\n",
        "*   La transferencia de información entre el *CPU* y la tarjeta gráfica genera un cuello de botella, revisar **ley de Amdhal**.\n",
        "*   No es posible ejecutar algoritmos recursivos.\n",
        "*   La unidad mínima de bloques debe ser de 32 threads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WogAVZ_eNKKm"
      },
      "source": [
        "# *CUDA*\n",
        "\n",
        "*CUDA* al igual que *OpenMP* y *MPI* es un *API* escrito para el lenguaje *C/C++*, lo que significa que es un conjunto de funciones y directivas que permiten procesar los datos haciendo uso de los *GPU's* disponibles.\n",
        "\n",
        "Debido a la naturaleza de *CUDA* y sobretodo a que es desarrollada por la empresa *Nvidia*, la instalación de *CUDA* requiere unos cuantos pasos adicionales, que de manera resumida se listan a continuación:\n",
        "\n",
        "1.   **Identificar el modelo de la tarjeta gráfica**: este paso es muy importante ya que de esto depende el resto del procedimiento.\n",
        "2.   **Descargar e instalar los controladores**: una vez que se conoce el modelo de la tarjeta gráfica es necesario descargar del sitio de [*Nvidia*](https://www.nvidia.com/Download/Find.aspx?lang=es) los controladores compatibles con el modelo de tarjeta.\n",
        "3.   **Descargar e instalar *CUDA***: con base en la versión de los controladores de Nvidia instalados, es necesario identificar la versión compatible de [*CUDA*](https://developer.nvidia.com/cuda-downloads), descargar e instalar la misma.\n",
        "\n",
        "Una vez que se instaló *CUDA*, ya es posible compilar y ejecutar código escrito mediante este *API*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9kPwGmoNPbY"
      },
      "source": [
        "## Funcionamiento de *CUDA*\n",
        "\n",
        "A diferencia de los *API's* anteriores (*OmpenMP, MPI*) *CUDA* se maneja un poco diferente.\n",
        "\n",
        "En *CUDA* toda sección de código en paralelo se ejecuta mediante un ***Kernel***, es decir *Kernel* = Función.\n",
        "\n",
        "Un *Kernel* se ejecuta en paralelo mediante *threads* (hilos) de *GPU*.\n",
        "\n",
        "Cada *Kernel* esta dividido en ***Blocks*** de una, dos o tres dimensiones de *threads*.\n",
        "\n",
        "Finalmente un ***Grid*** puede ser una arreglo de una, dos o tres dimensiones de *blocks*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/total.png?raw=1\" width=\"600\">\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6Z7lCWUN7kY"
      },
      "source": [
        "# Jerarquía de memoria\n",
        "\n",
        "Dadas los elementos del funcionamiento de *CUDA*, se tienen diferentes tipos de memoria y acceso a la misma, veamos.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/master/Imagenes/Cuda/memtotal.png?raw=1\" width=\"600\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UGHISwQOkgt"
      },
      "source": [
        "# *API CUDA*\n",
        "\n",
        "Los identificadores de funciones más importantes que se tienen en *CUDA* son los siguientes.\n",
        "\n",
        "1. __global__: indica que una función es de tipo *kernel*, es decir que sera ejecutada en un dispositivo (*GPU*) y solo puede ser llamada desde el host. Al momento de ser invocada se genera un *grid* de bloques con un número fijo e igual de *threads*.\n",
        "\n",
        "2. __device__: es una función que solo puede ser llamada desde un dispositivo, es decir que esta función solo puede ser invocada desde un *kernel* o desde otra función de tipo *device*.\n",
        "\n",
        "3. __host__: indica que esta función pertenece al host es decir que esta función solo puede ser ejecutada en el *host*. En otras palabras es una función de *C/C++* tradicional.\n",
        "\n",
        "Algunas de las variables reservadas de *CUDA* y que son de gran utilidad son las siguientes:\n",
        "\n",
        "*   **gridDim**: indica las dimensiones del *grid*.\n",
        "*   **blockIdx**: indica el identificador del Bloque de un *grid*.\n",
        "*   **blockDim**: contiene las dimensiones de un *block*.\n",
        "*   **threadIdx**: contiene el identificador del *thread* dentro del *block*.\n",
        "\n",
        "Es importante notar que todas estas variables contienen componentes en $X, Y$ y $Z$. Los *grids* y los *bloques* pueden ser de 1, 2 o 3 dimensiones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LAe7md8O68I"
      },
      "source": [
        "# Compilación y Ejecución\n",
        "\n",
        "Una vez instaladas las herramientas necesarias, solo es necesario usar las banderas (*flags*) correctas al momento de compilar y ejecutar algún programa escrito usando *CUDA*.\n",
        "\n",
        "La manera en la que se compila (revisión sintáctica) y se ejecuta un programa codificado mediante *CUDA*, es muy similar a como se compila y se ejecuta cualquier programa escrito en *C/C++*.\n",
        "\n",
        "Veamos como se compila y ejecuta tanto en *google colab*, como en un equipo local."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bivnzcwAVcOF"
      },
      "source": [
        "## *CUDA* en *Google Colab*\n",
        "\n",
        "Normalmente una vez iniciada la sesión de *google colab*, esta ya cuenta con todo lo necesario para compilar y ejecutar un programa usando *CUDA*, unicamente se tiene que habilitar el uso de los *GPU's*.\n",
        "\n",
        "Para habilitar el uso del *GPU*, es necesario dar click en *Entorno de ejecución -> Cambiar tipo de entorno de ejecución*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/ProgramacionEnParalelo/blob/desarrollo/Imagenes/CUDA/seleccion.png?raw=1\" width=\"400\">\n",
        "</center>\n",
        "\n",
        "Finalmente seleccionar *GPU*.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/ProgramacionEnParalelo/blob/desarrollo/Imagenes/CUDA/gpu.png?raw=1\" width=\"500\">\n",
        "</center>\n",
        "\n",
        "\n",
        "Para poder usar directamente *CUDA* en *google colab* es necesario hacer un *downgrade* de la versión actual de *CUDA* (desinstalar versión actual e instalar versión anterior), eso lo hacemos con la siguiente celda de código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLTC9k3NrQg_"
      },
      "source": [
        "Una vez que se actualizo el entorno de ejecución, lo siguiente es instalar el *plugin* neceario para compilar y ejecutar código de *CUDA* en *google colab*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyC-bWQidzPB",
        "outputId": "fb893c2b-c35c-4ac4-ad20-771e20da8fc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-jgugmywi\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-jgugmywi\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvcc4jupyter\n",
            "  Building wheel for nvcc4jupyter (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvcc4jupyter: filename=nvcc4jupyter-1.2.1-py3-none-any.whl size=10742 sha256=fdb30b442a03a2b978d94f242f4f3839c3b517111d4c5bcc31496f40b5aacbba\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-diou9wsc/wheels/7d/b9/66/459b9938664e6a93d1a85323ec52f7e51cd7265d253410a7d8\n",
            "Successfully built nvcc4jupyter\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgxkaJVJfnB9"
      },
      "source": [
        "Ya con el *plugin* instalado, se carga este plugin en la sesión actual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYfCwyYTdK_R",
        "outputId": "8125c985-3daf-4bc0-d44c-a94db5e5f19c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpdrsqe1b_\".\n"
          ]
        }
      ],
      "source": [
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwUQT_r0fYmh"
      },
      "source": [
        "Antes de escribir el código solo resta agregar *%%cu* en el encabezado de cualquier celda de código que contenga *CUDA* para que esta sea ejecutada como si fuera cualquier celda de código común."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxP4ZZ17dDGz",
        "outputId": "4e322482-4b09-428f-b227-c73749934e9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/host_config.h:50,\n",
            "                 from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:78,\n",
            "                 from <command-line>:\n",
            "/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:119:2: error: #error -- unsupported GNU version! gcc versions later than 7 are not supported!\n",
            "  119 | #error -- unsupported GNU version! gcc versions later than 7 are not supported!\n",
            "      |  ^~~~~\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# necesario para compilar codigo de CUDA\n",
        "%%cuda\n",
        "\n",
        "// Biblioteca de entrada/salida\n",
        "#include <stdio.h>\n",
        "// Definicion de un kernel\n",
        "__global__ void helloFromGPU(void){\n",
        "        printf(\"Hola mundo desde el GPU! threadIdx.x=%d\\n\", threadIdx.x);\n",
        "}\n",
        "\n",
        "// Funcion principal\n",
        "int main(void){\n",
        "   // Hola desde el CPU\n",
        "   printf(\"Hola mundo desde el CPU!\\n\");\n",
        "\n",
        "   // Llamada al kernel que se ejecuta en el GPU\n",
        "   helloFromGPU <<<1, 10>>>();\n",
        "   // Se liberan los recursos utilizados\n",
        "   cudaDeviceReset();\n",
        "   return(0);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5UDZ9GxPAhX"
      },
      "source": [
        "## *CUDA* en equipo local\n",
        "\n",
        "Una vez instalado el *API* de *CUDA* para el respectivo hardware la forma de compilar y ejecutar código de *CUDA* es muy similar al lenguaje *C/C++*.\n",
        "\n",
        "Supongamos que ya se cuenta con el código fuente de \"hola mundo\" para *CUDA* (helloCUDA.cu)\n",
        "\n",
        "*   Para compilar: *\\$nvcc helloCUDA.cu -o (helloCUDA.o) codigo*\n",
        "\n",
        "El comando anterior compila (y en caso de no haber errores) y genera un archivo ejecutable (binario) que puede ser ejecutado de la siguiente manera:\n",
        "\n",
        "*   Para ejecutar: *\\$./codigo*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfteTdToPWyV"
      },
      "source": [
        "# Glosario\n",
        "\n",
        "Dispositivo: En el contexto de *CUDA* un dispositivo hace referencia a un *GPU*.\n",
        "\n",
        "Asíncrono: En computación un evento (proceso) asíncrono es aquel no tiene correspondencia temporal con otro evento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHU_RuNSPbRQ"
      },
      "source": [
        "# Referencias\n",
        "\n",
        "1. Tolga Soyata: GPU Parallel Program Development Using Cuda.\n",
        "2. https://fisica.cab.cnea.gov.ar/gpgpu/images/clases/clase_1_cuda.pdf\n",
        "3. Dongarra Foster: Source Book of parallel computing."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CUDA_SCP.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}